{{- if .Values.components.otel.enabled }}
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.122.1
          command:
          - /otelcol-contrib
          - --config=/etc/otelcol-config/base.yaml
          - --set
          - receivers::otlp::protocols::http::endpoint=0.0.0.0:8335
          volumeMounts:
            - name: otel-collector-config
              mountPath: /etc/otelcol-config
          ports:
            - containerPort: 4317
              name: otlp-grpc
              protocol: TCP
            - containerPort: 4318
              name: otlp-http
              protocol: TCP
            - containerPort: 8335
              name: otlp-receiver
              protocol: TCP
      volumes:
        - name: otel-collector-config
          configMap:
            name: otel-collector-config
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
spec:
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      protocol: TCP
      port: 4317
      targetPort: 4317
    - name: otlp-http
      protocol: TCP
      port: 4318
      targetPort: 4318
    - name: otlp-receiver
      protocol: TCP
      port: 8335
      targetPort: 8335
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kagenti.labels" . | nindent 4 }}
data:
  base.yaml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318 # overriden by the server

    exporters:
      debug:
        verbosity: detailed
      otlp/phoenix:
        endpoint: phoenix:4317
        tls:
          insecure: true
      {{- if $.Values.components.mlflow.enabled }}
      otlphttp/mlflow:
        endpoint: http://mlflow:5000
        tls:
          insecure: true
      {{- end }}

    processors:
      memory_limiter:
        check_interval: 1s
        limit_mib: 1000
      batch:
      filter/phoenix:
        traces:
          span:
            ## Filter for openinference packages
            #### Python format `openinference.instrumentation.${package_name}`
            #### - crewAI exception `crewai.telemetry`
            #### Javascript format `@arizeai/openinference-instrumentation-${packageName}`
            - not(IsMatch(instrumentation_scope.name, "^openinference\\.instrumentation\\..*") or IsMatch(instrumentation_scope.name, "^@arizeai/openinference-instrumentation-.*") or instrumentation_scope.name == "crewai.telemetry")
      {{- if $.Values.components.mlflow.enabled }}
      # Transform processor to convert OpenTelemetry GenAI semantic conventions
      # to OpenInference format for compatibility with both Phoenix and MLflow
      # See: https://opentelemetry.io/docs/specs/semconv/gen-ai/
      # See: https://github.com/Arize-ai/openinference
      transform/genai_to_openinference:
        trace_statements:
          - context: span
            statements:
              # Convert GenAI model name to OpenInference format
              - set(attributes["llm.model_name"], attributes["gen_ai.request.model"]) where attributes["gen_ai.request.model"] != nil
              - set(attributes["llm.model_name"], attributes["gen_ai.response.model"]) where attributes["gen_ai.response.model"] != nil and attributes["llm.model_name"] == nil
              # Convert GenAI token counts to OpenInference format
              - set(attributes["llm.token_count.prompt"], attributes["gen_ai.usage.input_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil
              - set(attributes["llm.token_count.completion"], attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.output_tokens"] != nil
              - set(attributes["llm.token_count.total"], attributes["gen_ai.usage.input_tokens"] + attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil and attributes["gen_ai.usage.output_tokens"] != nil
              # Convert GenAI system to OpenInference provider
              - set(attributes["llm.provider"], attributes["gen_ai.system"]) where attributes["gen_ai.system"] != nil
              # Convert GenAI invocation parameters
              - set(attributes["llm.invocation_parameters"], Concat(["{\"temperature\":", Concat([attributes["gen_ai.request.temperature"], "}"])], "")) where attributes["gen_ai.request.temperature"] != nil
      {{- end }}

    extensions:
      health_check:

    service:
      extensions: [ health_check ]
      pipelines:
        traces/phoenix:
          receivers: [ otlp ]
          processors: [ memory_limiter, filter/phoenix, batch ]
          exporters: [ otlp/phoenix ]
        {{- if $.Values.components.mlflow.enabled }}
        traces/mlflow:
          receivers: [ otlp ]
          processors: [ memory_limiter, transform/genai_to_openinference, batch ]
          exporters: [ otlphttp/mlflow ]
        {{- end }}
{{- end }}
