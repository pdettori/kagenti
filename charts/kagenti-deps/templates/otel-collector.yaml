{{- if .Values.components.otel.enabled }}
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-collector
  template:
    metadata:
      labels:
        app: otel-collector
    spec:
      serviceAccountName: otel-collector
      containers:
        - name: otel-collector
          image: otel/opentelemetry-collector-contrib:0.122.1
          command:
          - /otelcol-contrib
          - --config=/etc/otelcol-config/base.yaml
          - --set
          - receivers::otlp::protocols::http::endpoint=0.0.0.0:8335
          {{- if and $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
          env:
            - name: MLFLOW_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: {{ $.Values.mlflow.auth.secretName | default "mlflow-oauth-secret" }}
                  key: OIDC_CLIENT_ID
            - name: MLFLOW_CLIENT_SECRET
              valueFrom:
                secretKeyRef:
                  name: {{ $.Values.mlflow.auth.secretName | default "mlflow-oauth-secret" }}
                  key: OIDC_CLIENT_SECRET
            - name: KEYCLOAK_TOKEN_URL
              valueFrom:
                secretKeyRef:
                  name: {{ $.Values.mlflow.auth.secretName | default "mlflow-oauth-secret" }}
                  key: OIDC_TOKEN_URL
          {{- end }}
          volumeMounts:
            - name: otel-collector-config
              mountPath: /etc/otelcol-config
            {{- if and $.Values.openshift $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
            - name: trusted-ca
              mountPath: /etc/pki/ca-trust/extracted/pem
              readOnly: true
            - name: ingress-ca
              mountPath: /etc/pki/ingress-ca
              readOnly: true
            {{- end }}
          ports:
            - containerPort: 4317
              name: otlp-grpc
              protocol: TCP
            - containerPort: 4318
              name: otlp-http
              protocol: TCP
            - containerPort: 8335
              name: otlp-receiver
              protocol: TCP
      volumes:
        - name: otel-collector-config
          configMap:
            name: otel-collector-config
        {{- if and $.Values.openshift $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
        # Use OpenShift's pre-existing trusted CA bundle ConfigMap
        - name: trusted-ca
          configMap:
            name: config-trusted-cabundle
            items:
              - key: ca-bundle.crt
                path: tls-ca-bundle.pem
        # OpenShift ingress CA for external Keycloak HTTPS
        # Copied from openshift-config-managed/default-ingress-cert by pre-install Job
        - name: ingress-ca
          configMap:
            name: otel-ingress-ca
            items:
              - key: ca-bundle.crt
                path: ingress-ca.pem
        {{- end }}
---
apiVersion: v1
kind: Service
metadata:
  name: otel-collector
  namespace: {{ .Release.Namespace }}
  labels:
    app: otel-collector
    {{- include "kagenti.labels" . | nindent 4 }}
spec:
  selector:
    app: otel-collector
  ports:
    - name: otlp-grpc
      protocol: TCP
      port: 4317
      targetPort: 4317
    - name: otlp-http
      protocol: TCP
      port: 4318
      targetPort: 4318
    - name: otlp-receiver
      protocol: TCP
      port: 8335
      targetPort: 8335
  type: ClusterIP
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "kagenti.labels" . | nindent 4 }}
data:
  base.yaml: |
    receivers:
      otlp:
        protocols:
          http:
            endpoint: 0.0.0.0:4318 # overriden by the server

    exporters:
      debug:
        verbosity: detailed
      otlp/phoenix:
        endpoint: phoenix:4317
        tls:
          insecure: true
      {{- if $.Values.components.mlflow.enabled }}
      otlphttp/mlflow:
        # Use traces_endpoint to specify exact URL (endpoint would append /v1/traces)
        traces_endpoint: http://mlflow:5000/v1/traces
        tls:
          insecure: true
        headers:
          # Default experiment ID (0 = Default experiment)
          # Traces can override via x-mlflow-experiment-id attribute
          x-mlflow-experiment-id: "0"
        {{- if $.Values.mlflow.auth.enabled }}
        # Authenticate to MLflow using OAuth2 client credentials via Keycloak
        auth:
          authenticator: oauth2client/mlflow
        {{- end }}
      {{- end }}

    processors:
      memory_limiter:
        check_interval: 1s
        limit_mib: 1000
      batch:
      filter/phoenix:
        traces:
          span:
            ## Filter OUT A2A framework spans (span names starting with "a2a.")
            ## Keep: OpenInference, LangChain, CrewAI, and GenAI agent spans
            - IsMatch(name, "^a2a\\..*")
      # Transform GenAI semantic conventions to OpenInference format for Phoenix
      # See: https://opentelemetry.io/docs/specs/semconv/gen-ai/
      # See: https://github.com/Arize-ai/openinference
      transform/genai_to_openinference:
        trace_statements:
          - context: span
            statements:
              # Convert GenAI model name to OpenInference format
              - set(attributes["llm.model_name"], attributes["gen_ai.request.model"]) where attributes["gen_ai.request.model"] != nil
              - set(attributes["llm.model_name"], attributes["gen_ai.response.model"]) where attributes["gen_ai.response.model"] != nil and attributes["llm.model_name"] == nil
              # Convert GenAI token counts to OpenInference format
              - set(attributes["llm.token_count.prompt"], attributes["gen_ai.usage.input_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil
              - set(attributes["llm.token_count.completion"], attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.output_tokens"] != nil
              - set(attributes["llm.token_count.total"], attributes["gen_ai.usage.input_tokens"] + attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil and attributes["gen_ai.usage.output_tokens"] != nil
              # Convert GenAI system to OpenInference provider
              - set(attributes["llm.provider"], attributes["gen_ai.system"]) where attributes["gen_ai.system"] != nil
              - set(attributes["llm.system"], attributes["gen_ai.system"]) where attributes["gen_ai.system"] != nil
              # Convert GenAI invocation parameters (temperature)
              - set(attributes["llm.invocation_parameters"], Concat(["{\"temperature\":", Concat([attributes["gen_ai.request.temperature"], "}"], "")], "")) where attributes["gen_ai.request.temperature"] != nil
      {{- if $.Values.components.mlflow.enabled }}
      # Filter for MLflow: only keep agent spans, filter out A2A framework spans
      # A2A framework spans have names starting with "a2a."
      filter/mlflow:
        traces:
          span:
            # Filter OUT spans that match these conditions (A2A framework spans):
            # - Span names starting with "a2a." (A2A server framework)
            # - Spans without any GenAI/agent attributes from our instrumentation
            # Keep: spans with mlflow.spanInputs (our agent spans), LangChain, OpenInference
            - IsMatch(name, "^a2a\\..*")
      # Transform GenAI semantic conventions to MLflow-specific metadata
      # MLflow uses its own trace metadata format, not OpenInference
      # See: https://mlflow.org/blog/opentelemetry-tracing-support
      # See: https://mlflow.org/docs/latest/genai/tracing/opentelemetry/ingest/
      transform/genai_to_mlflow:
        trace_statements:
          - context: span
            statements:
              # === MLflow Span Inputs/Outputs (for Request/Response columns) ===
              # Map inputs from GenAI prompt or span input attributes
              - set(attributes["mlflow.spanInputs"], attributes["gen_ai.prompt"]) where attributes["gen_ai.prompt"] != nil
              - set(attributes["mlflow.spanInputs"], attributes["input.value"]) where attributes["input.value"] != nil and attributes["mlflow.spanInputs"] == nil

              # Map outputs from GenAI completion or span output attributes
              - set(attributes["mlflow.spanOutputs"], attributes["gen_ai.completion"]) where attributes["gen_ai.completion"] != nil
              - set(attributes["mlflow.spanOutputs"], attributes["output.value"]) where attributes["output.value"] != nil and attributes["mlflow.spanOutputs"] == nil

              # === MLflow Session/Conversation Tracking ===
              # Map GenAI conversation ID to MLflow session metadata
              # This enables the MLflow Sessions tab to group traces by conversation
              - set(resource.attributes["mlflow.trace.session"], attributes["gen_ai.conversation.id"]) where attributes["gen_ai.conversation.id"] != nil

              # === MLflow Span Type Classification ===
              # Set span types for MLflow trace visualization
              - set(attributes["mlflow.spanType"], "AGENT") where IsMatch(name, "^gen_ai\\.agent\\..*")
              - set(attributes["mlflow.spanType"], "LLM") where attributes["gen_ai.request.model"] != nil and attributes["mlflow.spanType"] == nil
              - set(attributes["mlflow.spanType"], "TOOL") where IsMatch(name, "^gen_ai\\.tool\\..*")

              # === MLflow Trace Name (for Trace name column) ===
              # Use the span name of agent spans as trace name
              - set(resource.attributes["mlflow.traceName"], name) where IsMatch(name, "^gen_ai\\.agent\\..*")
              - set(resource.attributes["mlflow.traceName"], attributes["gen_ai.agent.name"]) where attributes["gen_ai.agent.name"] != nil

              # === MLflow User (for User column) ===
              # Map from standard OTEL user attributes or service name
              - set(resource.attributes["mlflow.user"], attributes["enduser.id"]) where attributes["enduser.id"] != nil
              - set(resource.attributes["mlflow.user"], resource.attributes["service.name"]) where attributes["enduser.id"] == nil and resource.attributes["mlflow.user"] == nil

              # === MLflow Source (for Source column) ===
              - set(resource.attributes["mlflow.source"], "otel-collector")

              # === MLflow Chat Usage (for Tokens column) ===
              # MLflow expects chat_usage with nested token counts
              - set(attributes["mlflow.span.chat_usage.input_tokens"], attributes["gen_ai.usage.input_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil
              - set(attributes["mlflow.span.chat_usage.output_tokens"], attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.output_tokens"] != nil
              - set(attributes["mlflow.span.chat_usage.total_tokens"], attributes["gen_ai.usage.input_tokens"] + attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil and attributes["gen_ai.usage.output_tokens"] != nil

              # Also keep llm.token_usage.* format (fallback)
              - set(attributes["llm.token_usage.input_tokens"], attributes["gen_ai.usage.input_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil
              - set(attributes["llm.token_usage.output_tokens"], attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.output_tokens"] != nil
              - set(attributes["llm.token_usage.total_tokens"], attributes["gen_ai.usage.input_tokens"] + attributes["gen_ai.usage.output_tokens"]) where attributes["gen_ai.usage.input_tokens"] != nil and attributes["gen_ai.usage.output_tokens"] != nil
      {{- end }}

    extensions:
      health_check:
      {{- if and $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}
      oauth2client/mlflow:
        # OAuth2 client credentials for authenticating to MLflow
        client_id: ${env:MLFLOW_CLIENT_ID}
        client_secret: ${env:MLFLOW_CLIENT_SECRET}
        token_url: ${env:KEYCLOAK_TOKEN_URL}
        scopes: ["openid"]
        # Timeout for token requests
        timeout: 10s
        {{- if $.Values.openshift }}
        # Use OpenShift ingress CA for external Keycloak HTTPS endpoint
        # The ingress CA is copied from openshift-config-managed/default-ingress-cert
        tls:
          ca_file: /etc/pki/ingress-ca/ingress-ca.pem
        {{- end }}
      {{- end }}

    service:
      extensions: [ health_check{{ if and $.Values.components.mlflow.enabled $.Values.mlflow.auth.enabled }}, oauth2client/mlflow{{ end }} ]
      pipelines:
        traces/phoenix:
          receivers: [ otlp ]
          processors: [ memory_limiter, filter/phoenix, transform/genai_to_openinference, batch ]
          exporters: [ otlp/phoenix ]
        {{- if $.Values.components.mlflow.enabled }}
        traces/mlflow:
          receivers: [ otlp ]
          processors: [ memory_limiter, filter/mlflow, transform/genai_to_mlflow, batch ]
          exporters: [ debug, otlphttp/mlflow ]
        {{- end }}
{{- end }}
