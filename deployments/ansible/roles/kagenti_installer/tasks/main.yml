# 
# setup
#

- name: setup vars 
  ansible.builtin.import_tasks: 01_setup_vars.yaml

- name: setup cluster
  ansible.builtin.import_tasks: 02_setup_cluster.yaml

# 
#  Install Istio
#
- name: Install Istio helm charts for Kubernetes cluster
  block:
    - name: Install istio-base
      kubernetes.core.helm:
        release_name: istio-base
        chart_ref: "base"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: true
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
      register: istio_base_result
      retries: 3
      delay: 15
      until: istio_base_result is not failed

    - name: Install istiod
      kubernetes.core.helm:
        release_name: istiod
        chart_ref: "istiod"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: false
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
        values:
          profile: ambient
      register: istiod_result
      retries: 3
      delay: 15
      until: istiod_result is not failed

    - name: Install istio-cni
      kubernetes.core.helm:
        release_name: istio-cni
        chart_ref: "cni"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: false
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
        values:
          profile: ambient
      register: istio_cni_result
      retries: 3
      delay: 15
      until: istio_cni_result is not failed

    - name: Install ztunnel
      kubernetes.core.helm:
        release_name: ztunnel
        chart_ref: "ztunnel"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: false
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
      register: ztunnel_result
      retries: 3
      delay: 15
      until: ztunnel_result is not failed

    - name: Label istio namespace to allow shared gateway access
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ charts.istio.namespace }}"
        definition:
          metadata:
            labels:
              shared-gateway-access: "true"
        merge_type: strategic-merge
  when: (charts.istio.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Cert manager
#
- name: Install cert manager and wait for start
  block:
  - name: Install cert-manager
    kubernetes.core.k8s:
      state: present
      src: "https://github.com/cert-manager/cert-manager/releases/download/{{certManager.version}}/cert-manager.yaml"
    register: cert_manager_result
    retries: 3
    delay: 15
    until: cert_manager_result is not failed

  - name: Wait for cert manager webhook to start
    command: >-
      kubectl wait --for=condition=Available deployment -n cert-manager cert-manager-webhook --timeout=120s
  when: (certManager.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Kiali and Prometheus
#
- name: Install Kiali and Prometheus istio addons for k8s cluster
  block:
    - name: Apply prometheus manifest
      kubernetes.core.k8s:
        state: present
        src: "https://raw.githubusercontent.com/istio/istio/{{kiali.version}}/samples/addons/prometheus.yaml"
      register: prometheus_result
      retries: 3
      delay: 15
      until: prometheus_result is not failed

    - name: Apply kiali manifest
      kubernetes.core.k8s:
        state: present
        src: "https://raw.githubusercontent.com/istio/istio/{{kiali.version}}/samples/addons/kiali.yaml"
      register: kiali_result
      retries: 3
      delay: 15
      until: kiali_result is not failed
  when: (kiali.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Tekton
#
- name: Install tekton
  kubernetes.core.k8s:
    state: present
    src: "https://storage.googleapis.com/tekton-releases/pipeline/previous/{{tekton.version}}/release.yaml"
  register: tekton_result
  retries: 3
  delay: 15
  until: tekton_result is not failed
  when: (tekton.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Shipwright
#
- name: Install Shipwright Build
  ansible.builtin.import_tasks: 04_install_shipwright.yaml

#
#  Toolhive
#
- name: Install Toolhive CRDs
  kubernetes.core.helm:
    release_name: "{{ toolhiveOperator.release_name }}-crds"
    chart_ref: "oci://ghcr.io/stacklok/toolhive/toolhive-operator-crds"
    chart_version: "{{ toolhiveCRDs.version }}"
    release_namespace: "{{ toolhiveOperator.namespace }}"
    create_namespace: true
    state: present
    wait: true
    timeout: "{{ helm_wait_timeout }}s"
  register: toolhive_crds_result
  retries: 3
  delay: 15
  until: toolhive_crds_result is not failed
  when: toolhiveCRDs.enabled | default(false)

# TODO: Request Toolhive operator helm chart to support OpenShift SCC.
# The Toolhive operator chart hardcodes runAsUser: 1000 which conflicts with
# OpenShift's restricted SCC (requires UID from namespace range).
# Proper fix: The chart should either:
# 1. Not specify runAsUser and let OpenShift assign one from the namespace range
# 2. Add an openshift.enabled value that omits runAsUser or includes an SCC
# 3. Use a SecurityContextConstraints resource in the chart for OpenShift
# Workaround: We install with wait=false, grant privileged SCC to the SA
# (which helm created), then wait for the deployment separately.
- name: Install Toolhive Operator
  kubernetes.core.helm:
    release_name: "{{ toolhiveOperator.release_name }}"
    chart_ref: "oci://ghcr.io/stacklok/toolhive/toolhive-operator"
    chart_version: "{{ toolhiveOperator.version }}"
    release_namespace: "{{ toolhiveOperator.namespace }}"
    create_namespace: true
    state: present
    # On OpenShift (wait=false): Don't wait here because pods can't start until
    # SCC is granted. We grant SCC after this task and wait for rollout separately.
    # On Kubernetes (wait=true): Wait normally since there's no SCC requirement.
    wait: "{{ not (enable_openshift | default(false)) }}"
    timeout: "{{ helm_wait_timeout }}s"
  register: toolhive_operator_result
  retries: 3
  delay: 15
  until: toolhive_operator_result is not failed
  when: toolhiveOperator.enabled | default(false)

- name: Grant privileged SCC to Toolhive operator for OpenShift
  command: oc adm policy add-scc-to-user privileged -z toolhive-operator -n {{ toolhiveOperator.namespace }}
  when:
    - enable_openshift | default(false)
    - toolhiveOperator.enabled | default(false)

- name: Wait for Toolhive operator deployment to be ready on OpenShift
  command: >-
    kubectl rollout status deployment/toolhive-operator
    -n {{ toolhiveOperator.namespace }} --timeout=300s
  when:
    - enable_openshift | default(false)
    - toolhiveOperator.enabled | default(false)

#
#  Spire
#
- name: Install SPIRE 
  block:
    - name: Install spire-crds via helm
      kubernetes.core.helm:
        release_name: spire-crds
        chart_ref: spire-crds
        chart_repo_url: https://spiffe.github.io/helm-charts-hardened/
        chart_version: "{{charts.spire.crd.version}}"
        release_namespace: "{{ charts.spire.namespace }}"
        create_namespace: true
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
      register: spire_crds_result
      retries: 3
      delay: 15
      until: spire_crds_result is not failed

    - name: Install spire
      kubernetes.core.helm:
        release_name: spire
        chart_ref: spire
        chart_repo_url: https://spiffe.github.io/helm-charts-hardened/
        chart_version: "{{charts.spire.version}}"
        release_namespace: "{{ charts.spire.namespace }}"
        create_namespace: true
        state: present
        wait: false
        timeout: "{{ helm_wait_timeout }}s"
        values: "{{ (charts['spire'] | default({}))['values'] | default({}) }}"
      register: spire_result
      retries: 3
      delay: 15
      until: spire_result is not failed
  when: charts.spire.enabled | default(false)

#
#  Gateway API
#
- name: Install Gateway API
  block:
    - name: Check for gateway CRD
      command: kubectl get crd gateways.gateway.networking.k8s.io
      register: gateway_crd_check
      failed_when: false
      changed_when: false

    - name: Apply gateway-api manifest
      kubernetes.core.k8s:
        state: present
        src: "https://github.com/kubernetes-sigs/gateway-api/releases/download/{{gatewayApi.version}}/standard-install.yaml"
      register: gateway_api_result
      retries: 3
      delay: 15
      until: gateway_api_result is not failed
      when: gateway_crd_check.rc != 0
  when: gatewayApi.enabled | default(false)

# Define reusable regex for detecting local chart paths (starts with './', '/', or '../')
- name: Define local chart path regex
  set_fact:
    local_chart_path_regex: '^\\.|^/|^../'

#
#  kagenti-deps
#
- name: Run helm dependency update for kagenti-deps when chart is local
  command: helm dependency update "{{ (charts['kagenti-deps'] | default({})).get('chart','') }}"
  args:
    chdir: "{{ playbook_dir }}"
  when:
    - (charts['kagenti-deps'] | default({})).get('chart') is defined
    - (charts['kagenti-deps'] | default({})).get('dependency_update', true) | bool
    - (charts['kagenti-deps'] | default({})).get('chart') is search(local_chart_path_regex)
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool
 
- name: Install/upgrade kagenti-deps chart
  kubernetes.core.helm:
    release_name: "{{ (charts['kagenti-deps'] | default({})).get('release_name', 'kagenti-deps') }}"
    chart_ref: "{{ (charts['kagenti-deps'] | default({})).get('chart') }}"
    chart_repo_url: "{{ (charts['kagenti-deps'] | default({})).get('repo_url', omit) }}"
    release_namespace: "{{ (charts['kagenti-deps'] | default({})).get('namespace', 'kagenti-system') }}"
    state: present
    create_namespace: true
    wait: true
    timeout: "{{ helm_wait_timeout }}s"
    values: >-
      {{ (((charts['kagenti-deps'] | default({})).get('values') ) | default({}))
        | combine((global_values_merged | default({})), recursive=True)
        | combine((((secret_values.get('charts', {}) | default({})).get('kagenti-deps', {}) ).get('values', {})) , recursive=True)
        | combine((secret_values.get('global', {}) | default({})), recursive=True)
        | combine({'openshift': enable_openshift}, recursive=True) }}
    values_files: >
      {{ (global_value_files | default([]) | list) +
        (((charts['kagenti-deps'] | default({}))['values_files'] | default([])) | list)
      }}
  when:
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool
    - (charts['kagenti-deps'] | default({})).get('chart') is defined

- name: Label kagenti-system namespace for shared gateway access
  kubernetes.core.k8s:
      api_version: v1
      kind: Namespace
      name: "{{ (charts['kagenti-deps'] | default({})).get('namespace', 'kagenti-system') }}"
      definition:
        metadata:
          labels:
            shared-gateway-access: "true"
      merge_type: strategic-merge
  when:
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool
    - (charts['kagenti-deps'] | default({})).get('chart') is defined

# TODO: Move TektonConfig patching to kagenti-operator
# The kagenti-operator creates PipelineRuns for AgentBuilds, so it has the
# context to know what Tekton settings are needed. The operator should:
# 1. Check if running on OpenShift (detect TektonConfig CRD)
# 2. Patch TektonConfig to set pipeline.set-security-context: true
# 3. This ensures proper fsGroup handling for PVC permissions in build pods
# For now, we patch it in the installer as a workaround.
- name: Patch TektonConfig for OpenShift build permissions
  block:
    - name: Wait for TektonConfig to exist
      command: kubectl get tektonconfig config
      register: tektonconfig_check
      retries: 30
      delay: 10
      until: tektonconfig_check.rc == 0
      changed_when: false

    - name: Patch TektonConfig to set security context
      kubernetes.core.k8s:
        api_version: operator.tekton.dev/v1alpha1
        kind: TektonConfig
        name: config
        definition:
          spec:
            pipeline:
              set-security-context: true
        merge_type: merge
  when: enable_openshift | default(false)

# Wait for Istio ambient mesh components to be ready on OpenShift.
# The Istio, IstioCNI, and ZTunnel CRs are created as helm post-install hooks,
# and ztunnel pods may start before istiod is ready, causing certificate issues.
- name: Wait for Istio ambient mesh to be ready on OpenShift
  block:
    - name: Wait for istiod deployment to be ready
      command: >-
        kubectl rollout status deployment/istiod
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}
        --timeout=300s
      register: istiod_ready
      retries: 3
      delay: 30
      until: istiod_ready.rc == 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('istio', {}).get('enabled', false) | bool

# On OpenShift AI clusters, there may be a pre-existing Istio (openshift-gateway) that
# creates CA ConfigMaps in all namespaces. To avoid CA conflicts between our istiod
# and openshift-gateway istiod, we use the "Shared Trust Pattern" - copying their CA
# to our istiod so both create identical ConfigMaps.
#
# TODO: Implement proper multi-mesh trust based on Istio deployment models.
# The current approach (copying CA secret) is a workaround. A cleaner solution
# would follow Istio's "Shared control plane (single-cluster)" or "Multi-primary"
# trust models as documented at:
# https://istio.io/latest/docs/ops/deployment/deployment-models/
#
# Options to consider:
# 1. Plug-in CA certificates: Configure both istiods to use the same external CA
#    https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/
# 2. Cross-cluster trust with intermediate CAs: Use cacerts Secret with proper chain
# 3. External CA integration (cert-manager): Single source of truth for certificates
- name: Share CA with openshift-gateway Istio (OpenShift AI clusters)
  block:
    - name: Check if openshift-gateway Istio CA exists
      command: kubectl get secret istio-ca-secret -n openshift-ingress
      register: og_ca_check
      failed_when: false
      changed_when: false

    - name: Copy openshift-gateway CA to istio-system (Shared Trust Pattern)
      shell: |
        kubectl get secret istio-ca-secret -n openshift-ingress -o yaml | \
          sed 's/namespace: openshift-ingress/namespace: {{ (charts["kagenti-deps"] | default({})).get("values", {}).get("istio", {}).get("namespace", "istio-system") }}/' | \
          grep -v "resourceVersion:\|uid:\|creationTimestamp:\|ownerReferences:" | \
          kubectl apply -f -
      when: og_ca_check.rc == 0

    - name: Restart istiod to use shared CA
      command: >-
        kubectl rollout restart deployment/istiod
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}
      when: og_ca_check.rc == 0

    - name: Wait for istiod rollout with shared CA
      command: >-
        kubectl rollout status deployment/istiod
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}
        --timeout=120s
      when: og_ca_check.rc == 0

    - name: Delete old CA ConfigMaps to force recreation with shared CA
      command: >-
        kubectl delete configmap istio-ca-root-cert
        -n {{ item }} --ignore-not-found
      loop:
        - "{{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('ztunnel', {}).get('namespace', 'istio-ztunnel') }}"
        - "{{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}"
        - "{{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('cni', {}).get('namespace', 'istio-cni') }}"
        - "{{ (charts['kagenti-deps'] | default({})).get('namespace', 'kagenti-system') }}"
      ignore_errors: true
      when: og_ca_check.rc == 0

    - name: Wait for istiod to recreate CA ConfigMaps with shared CA
      pause:
        seconds: 10
      when: og_ca_check.rc == 0

    - name: Restart ztunnel to pick up shared CA certificates
      command: >-
        kubectl rollout restart daemonset/ztunnel
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('ztunnel', {}).get('namespace', 'istio-ztunnel') }}
      when: og_ca_check.rc == 0

    - name: Wait for ztunnel rollout to complete
      command: >-
        kubectl rollout status daemonset/ztunnel
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('ztunnel', {}).get('namespace', 'istio-ztunnel') }}
        --timeout=300s
      register: ztunnel_ready
      retries: 3
      delay: 30
      until: ztunnel_ready.rc == 0
      when: og_ca_check.rc == 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('istio', {}).get('enabled', false) | bool

#
#  Setup DNS for registry in kind
#
- name: setup kind registry dns
  ansible.builtin.import_tasks: 03_setup_kind_registry_dns.yaml

#
#  MCP Gateway
#
- name: Install/upgrade mcp-gateway chart
  kubernetes.core.helm:
    release_name: mcp-gateway
    chart_ref: oci://ghcr.io/kagenti/charts/mcp-gateway
    chart_version: "{{ charts.mcpGateway.version }}"
    release_namespace: "{{ charts.mcpGateway.namespace }}"
    create_namespace: true
    state: present
    wait: false
    timeout: "{{ helm_wait_timeout }}s"
    values: >-
         {{ (((charts['mcpGateway'] | default({})).get('values')) | default({}))
          | combine((global_values_merged | default({})), recursive=True)
          | combine((((secret_values.get('charts', {}) | default({})).get('mcpGateway', {}) ).get('values', {})) , recursive=True)
          | combine((secret_values.get('global', {}) | default({})), recursive=True)
          | combine({'openshift': enable_openshift}, recursive=True) }}
  when: charts.mcpGateway.enabled | default(false)

#
#  Install kagenti
#
- name: Compute latest kagenti tag 
  block:
    - name: Lookup latest tag from remote git
      shell: |
        git ls-remote --tags --sort="v:refname" https://github.com/kagenti/kagenti.git | tail -n1 | sed 's|.*refs/tags/||; s/\^{}//'
      register: kagenti_latest_tag_cmd
      changed_when: false

    - name: Set kagenti_latest_tag fact
      set_fact:
        kagenti_latest_tag: "{{ kagenti_latest_tag_cmd.stdout }}"
  when: charts.kagenti.enabled | default(false)

- name: Run helm dependency update for kagenti when chart is local and dependency_update requested
  command: helm dependency update "{{ (charts['kagenti'] | default({})).get('chart','') }}"
  args:
    chdir: "{{ playbook_dir }}"
  when:
    - (charts['kagenti'] | default({})).get('chart') is defined
    - (charts['kagenti'] | default({})).get('dependency_update', true) | bool
    - (charts['kagenti'] | default({})).get('chart') is search(local_chart_path_regex)
    - (charts['kagenti'] | default({})).get('enabled', false) | bool

- name: Install/upgrade Kagenti chart and print instructions
  block:
  - name: Install/upgrade Kagenti chart
    kubernetes.core.helm:
      release_name: "{{ (charts['kagenti'] | default({})).get('release_name', 'kagenti') }}"
      chart_ref: "{{ (charts['kagenti'] | default({})).get('chart') }}"
      chart_repo_url: "{{ (charts['kagenti'] | default({})).get('repo_url', omit) }}"
      kubeconfig: ""
      release_namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"
      state: present
      create_namespace: false
      wait: true
      timeout: "{{ helm_wait_timeout }}s"
      values: >-
        {{ (((charts['kagenti'] | default({})).get('values')) | default({}))
          | combine({'ui': {'frontend': {'tag': kagenti_latest_tag}, 'backend': {'tag': kagenti_latest_tag}}}, recursive=True)
          | combine((global_values_merged | default({})), recursive=True)
          | combine((((secret_values.get('charts', {}) | default({})).get('kagenti', {}) ).get('values', {})) , recursive=True)
          | combine((secret_values.get('global', {}) | default({})), recursive=True)
          | combine({'openshift': enable_openshift}, recursive=True) }}
      values_files: >
        {{ (global_value_files | default([]) | list) +
          (((charts['kagenti'] | default({}))['values_files'] | default([])) | list)
        }}

  - name: Ensure kagenti-ui-oauth-secret-job is deleted
    kubernetes.core.k8s:
      state: absent
      api_version: batch/v1
      kind: Job
      name: kagenti-ui-oauth-secret-job
      namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"

  - name: Ensure kagenti-agent-oauth-secret-job is deleted
    kubernetes.core.k8s:
      state: absent
      api_version: batch/v1
      kind: Job
      name:  kagenti-agent-oauth-secret-job
      namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"

  # TODO: Move github-clone-step fixes to kagenti-operator.
  # The kagenti-operator creates the github-clone-step ConfigMap for Tekton pipelines.
  # On OpenShift with Istio ambient mode:
  # 1. HOME must be set to /tekton/home (git-init can't write to / on OpenShift)
  # 2. A 30s delay is needed for ztunnel to register the workload before DNS queries
  #    (ztunnel has a 5s timeout waiting for xds workload info from istiod)
  # The operator should include these fixes in the ConfigMap definition.
  # For now, we patch it in the installer as a workaround.
  - name: Wait for kagenti-operator to create github-clone-step ConfigMap
    command: kubectl get configmap github-clone-step -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}
    register: github_clone_step_check
    retries: 30
    delay: 5
    until: github_clone_step_check.rc == 0
    changed_when: false
    when: enable_openshift | default(false)

  - name: Patch github-clone-step ConfigMap for OpenShift and Istio ambient mode
    kubernetes.core.k8s:
      api_version: v1
      kind: ConfigMap
      name: github-clone-step
      namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"
      definition:
        data:
          task-spec.yaml: |
            params:
              - name: GITHUB_TOKEN
                type: string
                description: The Git repository URL with the token
                default: ""
              - name: repo-url
                type: string
                description: The Git repository URL without the token
              - name: revision
                type: string
                description: The Git revision to checkout
                default: "main"
            steps:
              - name: git-clone
                image: ghcr.io/tektoncd/github.com/tektoncd/pipeline/cmd/git-init:latest
                env:
                  - name: HOME
                    value: /tekton/home
                script: |
                  #!/bin/sh
                  # Wait for Istio ambient mesh ztunnel to register this workload.
                  # Without this delay, DNS queries fail because ztunnel times out
                  # waiting for workload info from istiod (xds).
                  echo "Waiting 30s for Istio ambient mesh to register this workload..."
                  sleep 30
                  /ko-app/git-init -url https://$(params.repo-url) -revision $(params.revision) -path /workspace/source
              - name: list-dir
                image: public.ecr.aws/docker/library/alpine:latest
                workingDir: $(workspaces.source.path)
                script: |
                    echo "Listing contents of workspace root ($(workspaces.source.path)): "
                    ls -la

            workspaces:
              - name: source
      merge_type: merge
    when: enable_openshift | default(false)

  when:
  - (charts['kagenti'] | default({})).get('enabled', false) | bool
  - (charts['kagenti'] | default({})).get('chart') is defined

# TODO: Move internal-registry-secret creation to kagenti-operator or kagenti helm chart.
# The kagenti-operator creates AgentBuilds with Tekton pipelines that push images
# to the OpenShift internal registry. The buildah step needs a dockerconfigjson-format
# secret with registry credentials. OpenShift auto-creates builder-dockercfg secrets
# but in dockercfg format, which buildah cannot use directly.
# For now, we create the internal-registry-secret in each agent namespace as a workaround.
- name: Create internal-registry-secret in agent namespaces for OpenShift
  block:
    - name: Create internal-registry-secret from builder SA token
      shell: |
        for ns in {{ (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | join(' ') }}; do
          # Get builder dockercfg and convert to dockerconfigjson format
          DOCKERCFG=$(kubectl get secret -n $ns -o name | grep builder-dockercfg | head -1 | xargs -I {} kubectl get {} -n $ns -o jsonpath='{.data.\.dockercfg}' | base64 -d)
          if [ -n "$DOCKERCFG" ]; then
            DOCKERCONFIGJSON=$(echo "{\"auths\": $DOCKERCFG}" | base64 | tr -d '\n')
            kubectl apply -f - <<EOF
        apiVersion: v1
        kind: Secret
        metadata:
          name: internal-registry-secret
          namespace: $ns
        type: kubernetes.io/dockerconfigjson
        data:
          .dockerconfigjson: $DOCKERCONFIGJSON
        EOF
            echo "Created internal-registry-secret in $ns"
          else
            echo "No builder-dockercfg found in $ns, skipping"
          fi
        done
      args:
        executable: /bin/bash
  when:
    - enable_openshift | default(false)
    - (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | length > 0

# TODO: Move spiffe-helper-config ConfigMap creation to kagenti-operator or toolhive operator.
# Toolhive-created StatefulSets expect this ConfigMap to exist for the spiffe-helper sidecar,
# but neither the Toolhive operator nor the kagenti operator creates it.
# For now, we create it in each agent namespace.
- name: Create spiffe-helper-config ConfigMap in agent namespaces
  block:
    - name: Apply spiffe-helper-config ConfigMap
      shell: |
        for ns in {{ (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | join(' ') }}; do
          kubectl apply -n $ns -f - <<EOF
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: spiffe-helper-config
        data:
          helper.conf: |
            agent_address = "/spiffe-workload-api/spire-agent.sock"
            cmd = ""
            cmd_args = ""
            svid_file_name = "/opt/svid.pem"
            svid_key_file_name = "/opt/svid_key.pem"
            svid_bundle_file_name = "/opt/svid_bundle.pem"
            jwt_svids = [{jwt_audience="kagenti", jwt_svid_file_name="/opt/jwt_svid.token"}]
            jwt_svid_file_mode = 0644
            include_federated_domains = true
        EOF
          echo "Created spiffe-helper-config in $ns"
        done
      args:
        executable: /bin/bash
  when:
    - (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | length > 0

# TODO: Move OAuth secret job ordering to kagenti helm chart.
# The kagenti-ui-oauth-secret-job needs the OpenShift Route to exist before it runs,
# because it reads the Route host to construct the OAuth callback URL. Currently,
# the Job and Route are created in parallel, causing a race condition where the Job
# may fail with "route not found" error. The Job eventually succeeds on retry, but
# the UI deployment may have already started with missing/stale OAuth credentials.
#
# Proper fix in charts/kagenti/templates/:
# 1. Add helm hook annotation to oauth-secret-job.yaml:
#    "helm.sh/hook": post-install,post-upgrade
#    "helm.sh/hook-weight": "10"  # Higher than Route weight
# 2. Ensure Route has lower hook-weight or is a regular resource (not a hook)
# 3. This guarantees Route exists before Job runs
#
# For now, we restart the UI deployment after the chart installs to pick up the
# OAuth secret that was created by the (eventually successful) Job.
- name: Restart kagenti-ui to pick up OAuth secret on OpenShift
  block:
    - name: Wait for OAuth secret job to complete
      command: >-
        kubectl wait --for=condition=complete job
        -l app.kubernetes.io/component=oauth-secret
        -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}
        --timeout=120s
      register: oauth_job_wait
      failed_when: false
      changed_when: false

    - name: Restart kagenti-ui deployment
      command: >-
        kubectl rollout restart deployment/kagenti-ui
        -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}

    - name: Wait for kagenti-ui rollout to complete
      command: >-
        kubectl rollout status deployment/kagenti-ui
        -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}
        --timeout=120s
  when:
    - enable_openshift | default(false)
    - (charts['kagenti'] | default({})).get('enabled', false) | bool

