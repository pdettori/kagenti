# 
# setup
#

- name: setup vars 
  ansible.builtin.import_tasks: 01_setup_vars.yaml

- name: setup cluster
  ansible.builtin.import_tasks: 02_setup_cluster.yaml

# 
#  Install Istio
#
- name: Install Istio helm charts for Kubernetes cluster
  block:
    - name: Install istio-base
      kubernetes.core.helm:
        release_name: istio-base
        chart_ref: "base"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: true
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
      register: istio_base_result
      retries: 3
      delay: 15
      until: istio_base_result is not failed

    - name: Install istiod
      kubernetes.core.helm:
        release_name: istiod
        chart_ref: "istiod"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: false
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
        values:
          profile: ambient
      register: istiod_result
      retries: 3
      delay: 15
      until: istiod_result is not failed

    - name: Install istio-cni
      kubernetes.core.helm:
        release_name: istio-cni
        chart_ref: "cni"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: false
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
        values:
          profile: ambient
      register: istio_cni_result
      retries: 3
      delay: 15
      until: istio_cni_result is not failed

    - name: Install ztunnel
      kubernetes.core.helm:
        release_name: ztunnel
        chart_ref: "ztunnel"
        chart_repo_url: https://istio-release.storage.googleapis.com/charts/
        chart_version: "{{charts.istio.version}}"
        release_namespace: "{{ charts.istio.namespace }}"
        create_namespace: false
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
      register: ztunnel_result
      retries: 3
      delay: 15
      until: ztunnel_result is not failed

    - name: Label istio namespace to allow shared gateway access
      kubernetes.core.k8s:
        api_version: v1
        kind: Namespace
        name: "{{ charts.istio.namespace }}"
        definition:
          metadata:
            labels:
              shared-gateway-access: "true"
        merge_type: strategic-merge
  when: (charts.istio.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Cert manager
#
- name: Install cert manager and wait for start
  block:
  - name: Install cert-manager
    kubernetes.core.k8s:
      state: present
      src: "https://github.com/cert-manager/cert-manager/releases/download/{{certManager.version}}/cert-manager.yaml"
    register: cert_manager_result
    retries: 3
    delay: 15
    until: cert_manager_result is not failed

  - name: Wait for cert manager webhook to start
    command: >-
      kubectl wait --for=condition=Available deployment -n cert-manager cert-manager-webhook --timeout=120s
  when: (certManager.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Kiali and Prometheus
#
- name: Install Kiali and Prometheus istio addons for k8s cluster
  block:
    - name: Apply prometheus manifest
      kubernetes.core.k8s:
        state: present
        src: "https://raw.githubusercontent.com/istio/istio/{{kiali.version}}/samples/addons/prometheus.yaml"
      register: prometheus_result
      retries: 3
      delay: 15
      until: prometheus_result is not failed

    - name: Apply kiali manifest
      kubernetes.core.k8s:
        state: present
        src: "https://raw.githubusercontent.com/istio/istio/{{kiali.version}}/samples/addons/kiali.yaml"
      register: kiali_result
      retries: 3
      delay: 15
      until: kiali_result is not failed
  when: (kiali.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Tekton
#
- name: Install tekton
  kubernetes.core.k8s:
    state: present
    src: "https://storage.googleapis.com/tekton-releases/pipeline/previous/{{tekton.version}}/release.yaml"
  register: tekton_result
  retries: 3
  delay: 15
  until: tekton_result is not failed
  when: (tekton.enabled | default(false)) and not (enable_openshift | default(false))

#
#  Shipwright
#
- name: Install Shipwright Build
  ansible.builtin.import_tasks: 04_install_shipwright.yaml

#
#  Spire (Helm-based for Kubernetes, or OpenShift < 4.19 where ZTWIM operator is not available)
#
- name: Install SPIRE via Helm charts
  block:
    - name: Display SPIRE Helm installation info
      debug:
        msg: |
          Installing SPIRE via Helm charts
          Reason: {{ spire_install_reason | default('Non-OpenShift Kubernetes cluster') }}
      when: use_spire_helm_chart | default(false)

    - name: Install spire-crds via helm
      kubernetes.core.helm:
        release_name: spire-crds
        chart_ref: spire-crds
        chart_repo_url: https://spiffe.github.io/helm-charts-hardened/
        chart_version: "{{charts.spire.crd.version}}"
        release_namespace: "{{ charts.spire.namespace }}"
        create_namespace: true
        state: present
        wait: true
        timeout: "{{ helm_wait_timeout }}s"
      register: spire_crds_result
      retries: 3
      delay: 15
      until: spire_crds_result is not failed

    - name: Install spire
      kubernetes.core.helm:
        release_name: spire
        chart_ref: spire
        chart_repo_url: https://spiffe.github.io/helm-charts-hardened/
        chart_version: "{{charts.spire.version}}"
        release_namespace: "{{ charts.spire.namespace }}"
        create_namespace: true
        state: present
        wait: false
        timeout: "{{ helm_wait_timeout }}s"
        values: "{{ (charts['spire'] | default({}))['values'] | default({}) }}"
      register: spire_result
      retries: 3
      delay: 15
      until: spire_result is not failed
  when: >
    (charts.spire.enabled | default(false)) and
    (not (enable_openshift | default(false)) or (use_spire_helm_chart | default(false)))

#
#  Gateway API
#
- name: Install Gateway API
  block:
    - name: Check for gateway CRD
      command: kubectl get crd gateways.gateway.networking.k8s.io
      register: gateway_crd_check
      failed_when: false
      changed_when: false

    - name: Apply gateway-api manifest
      kubernetes.core.k8s:
        state: present
        src: "https://github.com/kubernetes-sigs/gateway-api/releases/download/{{gatewayApi.version}}/standard-install.yaml"
      register: gateway_api_result
      retries: 3
      delay: 15
      until: gateway_api_result is not failed
      when: gateway_crd_check.rc != 0
  when: gatewayApi.enabled | default(false)

# Define reusable regex for detecting local chart paths (starts with './', '/', or '../')
- name: Define local chart path regex
  set_fact:
    local_chart_path_regex: '^\\.|^/|^../'

#
#  kagenti-deps
#
- name: Run helm dependency update for kagenti-deps when chart is local
  command: helm dependency update "{{ (charts['kagenti-deps'] | default({})).get('chart','') }}"
  args:
    chdir: "{{ playbook_dir }}"
  when:
    - (charts['kagenti-deps'] | default({})).get('chart') is defined
    - (charts['kagenti-deps'] | default({})).get('dependency_update', true) | bool
    - (charts['kagenti-deps'] | default({})).get('chart') is search(local_chart_path_regex)
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool

# Detect already-installed OLM operators on OpenShift.
# If an operator is already installed (not by this Helm chart), we skip installing it
# to avoid conflicts with existing OperatorGroups and Subscriptions.
- name: Detect already-installed OLM operators on OpenShift
  block:
    - name: Check if cert-manager operator is already installed via OLM (Succeeded state)
      shell: >-
        kubectl get csv -n cert-manager-operator -o json 2>/dev/null |
        jq -e '.items[] | select(.status.phase == "Succeeded")' > /dev/null 2>&1 &&
        kubectl get subscription -n cert-manager-operator -o json 2>/dev/null |
        jq -e '.items[] | select((.spec.name == "cert-manager-operator" or ((.status.currentCSV // "") | contains("cert-manager-operator"))) and (.metadata.annotations == null or .metadata.annotations["meta.helm.sh/release-name"] != "kagenti-deps"))' > /dev/null 2>&1
      args:
        executable: /bin/bash
      register: cert_manager_olm_check
      failed_when: false
      changed_when: false

    - name: Check if cert-manager OLM resources exist but are stuck/conflicting
      shell: >-
        kubectl get subscription -n cert-manager-operator -o name 2>/dev/null | grep -q . &&
        ! (kubectl get csv -n cert-manager-operator -o json 2>/dev/null |
           jq -e '.items[] | select(.status.phase == "Succeeded")' > /dev/null 2>&1)
      args:
        executable: /bin/bash
      register: cert_manager_olm_stuck_check
      failed_when: false
      changed_when: false

    - name: Check if cert-manager is already installed by other means (e.g., OpenShift Pipelines/Tekton)
      shell: >-
        kubectl get crd certificates.cert-manager.io > /dev/null 2>&1 &&
        (kubectl get pods -n cert-manager -l app.kubernetes.io/instance=cert-manager --field-selector=status.phase=Running -o name 2>/dev/null | grep -q .)
      args:
        executable: /bin/bash
      register: cert_manager_existing_check
      failed_when: false
      changed_when: false

    - name: Set cert-manager skip fact
      set_fact:
        skip_cert_manager_operator: "{{ cert_manager_olm_check.rc == 0 or cert_manager_existing_check.rc == 0 }}"
        cert_manager_olm_stuck: "{{ cert_manager_olm_stuck_check.rc == 0 }}"
        cert_manager_from_tekton: "{{ cert_manager_existing_check.rc == 0 and cert_manager_olm_check.rc != 0 }}"

    - name: Debug cert-manager detection
      debug:
        msg: >-
          cert-manager already installed: {{ skip_cert_manager_operator }}
          (OLM operator succeeded: {{ cert_manager_olm_check.rc == 0 }},
          OLM resources stuck: {{ cert_manager_olm_stuck | default(false) }},
          existing install e.g. Tekton/OpenShift Pipelines: {{ cert_manager_existing_check.rc == 0 }}).
          Will skip cert-manager operator installation and use existing.
      when: skip_cert_manager_operator | default(false)

    # Clean up stuck/conflicting cert-manager OLM resources when a working cert-manager exists elsewhere
    # This handles reinstall scenarios where:
    # 1. Previous install created OLM subscription for cert-manager operator
    # 2. Tekton/OpenShift Pipelines installed its own cert-manager
    # 3. OLM cert-manager CSV is stuck in Pending due to conflict
    - name: Clean up conflicting cert-manager OLM resources
      block:
        - name: Display cert-manager cleanup notice
          debug:
            msg: |
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
              ℹ️  CLEANING UP CONFLICTING CERT-MANAGER OLM RESOURCES
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

              Detected: cert-manager OLM operator subscription exists but CSV is not Succeeded
              Reason: Likely conflict with Tekton/OpenShift Pipelines cert-manager

              A working cert-manager installation was found (installed by Tekton/OpenShift Pipelines).
              Removing conflicting OLM resources to use the existing cert-manager.

              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

        - name: Delete cert-manager OLM Subscription
          kubernetes.core.k8s:
            api_version: operators.coreos.com/v1alpha1
            kind: Subscription
            name: cert-manager
            namespace: cert-manager-operator
            state: absent
          failed_when: false

        - name: Delete all CSVs in cert-manager-operator namespace
          shell: kubectl delete csv --all -n cert-manager-operator --ignore-not-found
          args:
            executable: /bin/bash
          failed_when: false

        - name: Delete cert-manager OperatorGroup
          kubernetes.core.k8s:
            api_version: operators.coreos.com/v1
            kind: OperatorGroup
            name: cert-manager-operator
            namespace: cert-manager-operator
            state: absent
          failed_when: false

        - name: Wait for OLM cleanup to complete
          pause:
            seconds: 5
      when:
        - cert_manager_olm_stuck | default(false)
        - cert_manager_from_tekton | default(false)
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # OpenShift Version Detection for SPIRE/ZTWIM Compatibility
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # ZTWIM (Zero Trust Workload Identity Manager) operator requires OCP 4.19+
    # This check detects the OpenShift version and auto-disables SPIRE if incompatible
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: Detect OpenShift cluster version
      shell: |
        # Try multiple methods to get OCP version
        if command -v oc &> /dev/null; then
          # Method 1: oc version (most reliable)
          oc version -o json 2>/dev/null | jq -r '.openshiftVersion // empty' && exit 0
        fi
        # Method 2: ClusterVersion resource
        kubectl get clusterversion version -o jsonpath='{.status.desired.version}' 2>/dev/null && exit 0
        # Method 3: Check for version in cluster info
        kubectl version --short 2>/dev/null | grep -oP 'Server Version: v\K[0-9]+\.[0-9]+' && exit 0
        # If all methods fail, return empty
        echo ""
      args:
        executable: /bin/bash
      register: ocp_version_raw
      failed_when: false
      changed_when: false

    - name: Parse OpenShift version
      set_fact:
        ocp_version: "{{ ocp_version_raw.stdout | trim }}"
        ocp_version_major: "{{ (ocp_version_raw.stdout | trim | regex_search('^([0-9]+)\\.([0-9]+)', '\\1', '\\2'))[0] | default('0') | int }}"
        ocp_version_minor: "{{ (ocp_version_raw.stdout | trim | regex_search('^([0-9]+)\\.([0-9]+)', '\\1', '\\2'))[1] | default('0') | int }}"
      when: ocp_version_raw.stdout | trim | length > 0

    - name: Display detected OpenShift version
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          OpenShift Version Detection
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Detected Version: {{ ocp_version | default('Unable to detect') }}
          Major: {{ ocp_version_major | default('N/A') }}
          Minor: {{ ocp_version_minor | default('N/A') }}

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # OpenShift Trust Domain Detection for SPIRE/ZTWIM
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # ZTWIM uses the OpenShift "apps" subdomain as its Trust Domain by default.
    # We detect this from the DNS cluster resource and pass it to the Helm chart.
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

    - name: Detect OpenShift apps domain for SPIRE trust domain
      shell: |
        BASE_DOMAIN=$(kubectl get dns cluster -o jsonpath='{.spec.baseDomain}' 2>/dev/null)
        if [ -n "$BASE_DOMAIN" ]; then
          echo "apps.${BASE_DOMAIN}"
        else
          echo ""
        fi
      args:
        executable: /bin/bash
      register: ocp_trust_domain_raw
      failed_when: false
      changed_when: false

    - name: Set OpenShift trust domain fact
      set_fact:
        ocp_trust_domain: "{{ ocp_trust_domain_raw.stdout | trim }}"
      when: ocp_trust_domain_raw.stdout | trim | length > 0

    - name: Display detected trust domain
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          OpenShift Trust Domain Detection
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          Trust Domain: {{ ocp_trust_domain }}
          This will be used for SPIRE/ZTWIM configuration.
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
      when: ocp_trust_domain is defined and ocp_trust_domain | length > 0

    - name: Warn if trust domain could not be detected
      debug:
        msg: |
          ⚠️  WARNING: Could not detect OpenShift trust domain from DNS cluster resource.
          SPIRE/ZTWIM will use the default trust domain from values file.
          To set manually, add to your values: spire.trustDomain: "apps.<your-base-domain>"
      when: ocp_trust_domain is not defined or ocp_trust_domain | length == 0

    - name: Check SPIRE/ZTWIM version compatibility
      set_fact:
        spire_version_compatible: >-
          {{ (ocp_version_major | int == 4 and ocp_version_minor | int >= 19) or
             (ocp_version_major | int > 4) }}
      when: 
        - ocp_version is defined
        - ocp_version_major is defined
        - ocp_version_minor is defined

    - name: Use SPIRE Helm chart on older OpenShift versions
      block:
        - name: Display SPIRE Helm chart fallback notice
          debug:
            msg: |
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
              ℹ️  USING SPIRE HELM CHART (OCP < 4.19)
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

              Current OpenShift Version: {{ ocp_version }}
              ZTWIM Operator Requires: 4.19.0 or higher

              The ZTWIM (Zero Trust Workload Identity Manager) operator is only available
              on OpenShift 4.19+. On older versions, SPIRE will be installed using the
              upstream Helm charts instead.

              ACTION: Installing SPIRE via Helm charts (same as non-OpenShift Kubernetes)

              Note: For full OLM-managed SPIRE, upgrade to OpenShift 4.19+
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

        - name: Set facts to use SPIRE Helm chart instead of ZTWIM operator
          set_fact:
            skip_spire_operator: true
            use_spire_helm_chart: true
            spire_install_method: "helm-chart"
            spire_install_reason: "OpenShift version {{ ocp_version }} < 4.19.0 (using Helm chart fallback)"
      when:
        - spire_version_compatible is defined
        - not (spire_version_compatible | bool)
        - (charts['kagenti-deps'].values.components.spire.enabled | default(false)) | bool

    - name: Confirm SPIRE compatibility
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✓ SPIRE/ZTWIM Version Check Passed
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          OpenShift Version: {{ ocp_version }}
          ZTWIM operator is supported on this cluster version.
          Checking if ZTWIM package is available in the operator catalog...
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
      when:
        - spire_version_compatible is defined
        - spire_version_compatible | bool
        - (charts['kagenti-deps'].values.components.spire.enabled | default(false)) | bool

    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # ZTWIM Package Availability Check
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    # Even if OCP version is compatible (4.19+), the ZTWIM operator may not be
    # available in the catalog (e.g., HyperShift clusters, missing entitlements).
    # Check if the packagemanifest exists before attempting subscription.
    # ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
    - name: Check if ZTWIM package is available in operator catalog
      shell: |
        # Check if packagemanifest exists for ZTWIM operator
        # This verifies the operator is actually in the catalog before subscribing
        kubectl get packagemanifest openshift-zero-trust-workload-identity-manager \
          -n openshift-marketplace -o name 2>/dev/null
      args:
        executable: /bin/bash
      register: ztwim_package_check
      failed_when: false
      changed_when: false
      when:
        - spire_version_compatible is defined
        - spire_version_compatible | bool
        - not (skip_spire_operator | default(false))
        - (charts['kagenti-deps'].values.components.spire.enabled | default(false)) | bool

    - name: Use SPIRE Helm chart when ZTWIM package not in catalog
      block:
        - name: Display ZTWIM package not found notice
          debug:
            msg: |
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
              ⚠️  ZTWIM OPERATOR NOT FOUND IN CATALOG
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

              OpenShift Version: {{ ocp_version }} (compatible with ZTWIM)
              Package: openshift-zero-trust-workload-identity-manager
              Catalog: redhat-operators

              The ZTWIM operator package was not found in the operator catalog.
              This can happen on:
              - HyperShift hosted clusters
              - Clusters without Red Hat OpenShift Platform Plus subscription
              - Clusters where the catalog hasn't fully synced yet

              ACTION: Installing SPIRE via Helm charts as fallback.

              Note: If you need the ZTWIM operator, verify:
              1. Your cluster has the required subscription/entitlement
              2. The catalog pods are healthy: kubectl get pods -n openshift-marketplace
              3. Try refreshing the catalog: kubectl delete pods -n openshift-marketplace -l olm.catalogSource=redhat-operators
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

        - name: Set facts to use SPIRE Helm chart (package not in catalog)
          set_fact:
            skip_spire_operator: true
            use_spire_helm_chart: true
            spire_install_method: "helm-chart"
            spire_install_reason: "ZTWIM package not found in redhat-operators catalog (using Helm chart fallback)"
      when:
        - spire_version_compatible is defined
        - spire_version_compatible | bool
        - ztwim_package_check is defined
        - ztwim_package_check.rc is defined
        - ztwim_package_check.rc != 0
        - not (skip_spire_operator | default(false))
        - (charts['kagenti-deps'].values.components.spire.enabled | default(false)) | bool

    - name: Confirm ZTWIM package is available
      debug:
        msg: |
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          ✓ ZTWIM Package Available
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
          OpenShift Version: {{ ocp_version }}
          ZTWIM operator package found in catalog.
          Proceeding with ZTWIM operator installation.
          ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
      when:
        - spire_version_compatible is defined
        - spire_version_compatible | bool
        - ztwim_package_check is defined
        - ztwim_package_check.rc is defined
        - ztwim_package_check.rc == 0
        - not (skip_spire_operator | default(false))
        - (charts['kagenti-deps'].values.components.spire.enabled | default(false)) | bool

    # Handle case when OpenShift version detection fails
    # Fall back to Helm chart installation since we can't verify ZTWIM compatibility
    - name: Handle version detection failure for SPIRE
      block:
        - name: Display version detection failure warning
          debug:
            msg: |
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
              ⚠️  UNABLE TO DETECT OPENSHIFT VERSION
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

              Cannot verify OpenShift version for SPIRE/ZTWIM compatibility.
              The ZTWIM operator requires OpenShift 4.19.0 or higher.

              ACTION: Installing SPIRE via Helm charts as fallback.

              Note: If you're running OCP 4.19+, you can manually install ZTWIM operator
              after verifying your version with: oc version
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

        - name: Set facts to use SPIRE Helm chart due to version detection failure
          set_fact:
            skip_spire_operator: true
            use_spire_helm_chart: true
            spire_install_method: "helm-chart"
            spire_install_reason: "Unable to detect OpenShift version - using Helm chart fallback"
      when:
        - ocp_version is not defined or (ocp_version | trim | length == 0)
        - (charts['kagenti-deps'].values.components.spire.enabled | default(false)) | bool


    - name: Check if SPIRE/ZTWIM operator is already installed
      shell: >-
        kubectl get csv -n zero-trust-workload-identity-manager -o json 2>/dev/null |
        jq -e '.items[] | select(.status.phase == "Succeeded")' > /dev/null 2>&1 &&
        kubectl get subscription -n zero-trust-workload-identity-manager -o json 2>/dev/null |
        jq -e '.items[] | select(.metadata.annotations == null or .metadata.annotations["meta.helm.sh/release-name"] != "kagenti-deps")' > /dev/null 2>&1
      args:
        executable: /bin/bash
      register: spire_operator_check
      failed_when: false
      changed_when: false

    - name: Set SPIRE operator skip fact (preserve version check result)
      set_fact:
        # Preserve skip_spire_operator if already set to true by version check
        # Otherwise, set based on whether SPIRE operator is already installed
        skip_spire_operator: "{{ skip_spire_operator | default(false) or spire_operator_check.rc == 0 }}"

    - name: Debug SPIRE operator detection
      debug:
        msg: >-
          SPIRE skip decision: {{ skip_spire_operator }}
          (version incompatible: {{ spire_disabled_reason | default('N/A') }},
          already installed: {{ spire_operator_check.rc == 0 }})
      when: skip_spire_operator | default(false)

    - name: Check if OpenShift Builds operator is already installed
      shell: >-
        kubectl get csv -n openshift-builds -o json 2>/dev/null |
        jq -e '.items[] | select(.status.phase == "Succeeded")' > /dev/null 2>&1 &&
        kubectl get subscription -n openshift-builds -o json 2>/dev/null |
        jq -e '.items[] | select(.metadata.annotations == null or .metadata.annotations["meta.helm.sh/release-name"] != "kagenti-deps")' > /dev/null 2>&1
      args:
        executable: /bin/bash
      register: builds_operator_check
      failed_when: false
      changed_when: false

    - name: Set OpenShift Builds operator skip fact
      set_fact:
        skip_builds_operator: "{{ builds_operator_check.rc == 0 }}"

    - name: Debug OpenShift Builds operator detection
      debug:
        msg: "OpenShift Builds operator already installed (not by Helm): {{ skip_builds_operator }}"
      when: skip_builds_operator | default(false)
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool

# Build operator skip overrides based on detection results.
# If an operator is already installed (not by Helm), we disable its component to avoid conflicts.
- name: Build operator skip overrides for kagenti-deps
  block:
    - name: Initialize operator overrides
      set_fact:
        kagenti_deps_operator_overrides: {}

    # Pass detected OCP version to Helm chart to bypass cluster lookup during helm diff/template
    # This is necessary because helm diff uses helm template which can't do cluster lookups
    - name: Add detected OCP version to overrides
      set_fact:
        kagenti_deps_operator_overrides: "{{ kagenti_deps_operator_overrides | combine({'ocpVersion': ocp_version}, recursive=True) }}"
      when: ocp_version is defined and ocp_version | trim | length > 0

    # Set SPIRE trust domain from detected OpenShift apps domain
    # This follows the ZTWIM convention of using apps.<baseDomain> as trust domain
    - name: Add SPIRE trust domain override
      set_fact:
        kagenti_deps_operator_overrides: "{{ kagenti_deps_operator_overrides | combine({'spire': {'trustDomain': ocp_trust_domain}}, recursive=True) }}"
      when: ocp_trust_domain is defined and ocp_trust_domain | trim | length > 0

    - name: Add cert-manager skip override
      set_fact:
        kagenti_deps_operator_overrides: "{{ kagenti_deps_operator_overrides | combine({'components': {'certManager': {'enabled': false}}}, recursive=True) }}"
      when: skip_cert_manager_operator | default(false)

    # When using SPIRE Helm chart (OCP < 4.19), set useSpireHelmChart flag
    # This tells kagenti-deps to skip ZTWIM operator but keeps SPIRE "enabled" for other dependencies
    - name: Add SPIRE Helm chart override (for OCP < 4.19)
      set_fact:
        kagenti_deps_operator_overrides: "{{ kagenti_deps_operator_overrides | combine({'useSpireHelmChart': true}, recursive=True) }}"
      when: use_spire_helm_chart | default(false)

    # When SPIRE operator should be completely skipped (not using Helm chart either)
    - name: Add SPIRE operator skip override (disable completely)
      set_fact:
        kagenti_deps_operator_overrides: "{{ kagenti_deps_operator_overrides | combine({'components': {'spire': {'enabled': false}}}, recursive=True) }}"
      when:
        - skip_spire_operator | default(false)
        - not (use_spire_helm_chart | default(false))

    - name: Add OpenShift Builds operator skip override
      set_fact:
        kagenti_deps_operator_overrides: "{{ kagenti_deps_operator_overrides | combine({'components': {'shipwright': {'enabled': false}}}, recursive=True) }}"
      when: skip_builds_operator | default(false)

    - name: Debug operator overrides
      debug:
        msg: "Operator overrides for kagenti-deps: {{ kagenti_deps_operator_overrides }}"
      when: kagenti_deps_operator_overrides | length > 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool

- name: Install/upgrade kagenti-deps chart
  kubernetes.core.helm:
    release_name: "{{ (charts['kagenti-deps'] | default({})).get('release_name', 'kagenti-deps') }}"
    chart_ref: "{{ (charts['kagenti-deps'] | default({})).get('chart') }}"
    chart_repo_url: "{{ (charts['kagenti-deps'] | default({})).get('repo_url', omit) }}"
    release_namespace: "{{ (charts['kagenti-deps'] | default({})).get('namespace', 'kagenti-system') }}"
    state: present
    create_namespace: true
    wait: true
    timeout: "{{ helm_wait_timeout }}s"
    values: >-
      {{ (((charts['kagenti-deps'] | default({})).get('values') ) | default({}))
        | combine((global_values_merged | default({})), recursive=True)
        | combine((((secret_values.get('charts', {}) | default({})).get('kagenti-deps', {}) ).get('values', {})) , recursive=True)
        | combine((secret_values.get('global', {}) | default({})), recursive=True)
        | combine({'openshift': enable_openshift}, recursive=True)
        | combine((kagenti_deps_operator_overrides | default({})), recursive=True) }}
    values_files: >
      {{ (global_value_files | default([]) | list) +
        (((charts['kagenti-deps'] | default({}))['values_files'] | default([])) | list)
      }}
  when:
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool
    - (charts['kagenti-deps'] | default({})).get('chart') is defined

- name: Label kagenti-system namespace for shared gateway access
  kubernetes.core.k8s:
      api_version: v1
      kind: Namespace
      name: "{{ (charts['kagenti-deps'] | default({})).get('namespace', 'kagenti-system') }}"
      definition:
        metadata:
          labels:
            shared-gateway-access: "true"
      merge_type: strategic-merge
  when:
    - (charts['kagenti-deps'] | default({})).get('enabled', false) | bool
    - (charts['kagenti-deps'] | default({})).get('chart') is defined

# Wait for cert-manager CRDs to be available on OpenShift.
# The cert-manager operator is installed via OLM subscription in kagenti-deps,
# but the CRDs take time to be created by the operator. The kagenti chart
# depends on Certificate and Issuer CRDs being available.
- name: Wait for cert-manager CRDs on OpenShift
  block:
    - name: Wait for cert-manager operator CSV to succeed
      command: >-
        kubectl get csv -n cert-manager-operator
        -o jsonpath='{.items[?(@.spec.displayName=="cert-manager Operator for Red Hat OpenShift")].status.phase}'
      register: cert_manager_csv_status
      retries: 30
      delay: 10
      until: cert_manager_csv_status.stdout == "Succeeded"
      failed_when: false
      changed_when: false

    - name: Display cert-manager CSV status on failure
      debug:
        msg: |
          Cert-manager CSV failed to reach 'Succeeded' state after 5 minutes.
          Current status: {{ cert_manager_csv_status.stdout | default('Unknown') }}
          
          Troubleshooting steps:
          1. Check operator logs: kubectl logs -n cert-manager-operator -l app=cert-manager-operator
          2. Check CSV details: kubectl describe csv -n cert-manager-operator
          3. Verify operator subscription: kubectl get subscription -n cert-manager-operator
      when: cert_manager_csv_status.stdout != "Succeeded"

    - name: Fail if CSV did not succeed
      fail:
        msg: "Cert-manager operator CSV failed to reach 'Succeeded' state. See troubleshooting steps above."
      when: cert_manager_csv_status.stdout != "Succeeded"

    - name: Wait for Certificate CRD to be available
      command: kubectl get crd certificates.cert-manager.io
      register: cert_crd_check
      retries: 20
      delay: 10
      until: cert_crd_check.rc == 0
      failed_when: false
      changed_when: false

    - name: Display Certificate CRD status on failure
      debug:
        msg: |
          Certificate CRD failed to become available after 3.3 minutes.
          Check cert-manager operator status: kubectl get pods -n cert-manager-operator
      when: cert_crd_check.rc != 0

    - name: Fail if Certificate CRD not available
      fail:
        msg: "Certificate CRD not available. Check cert-manager operator installation."
      when: cert_crd_check.rc != 0

    - name: Wait for Issuer CRD to be available
      command: kubectl get crd issuers.cert-manager.io
      register: issuer_crd_check
      retries: 20
      delay: 10
      until: issuer_crd_check.rc == 0
      failed_when: false
      changed_when: false

    - name: Display Issuer CRD status on failure
      debug:
        msg: |
          Issuer CRD failed to become available after 3.3 minutes.
          Check cert-manager operator status: kubectl get pods -n cert-manager-operator
      when: issuer_crd_check.rc != 0

    - name: Fail if Issuer CRD not available
      fail:
        msg: "Issuer CRD not available. Check cert-manager operator installation."
      when: issuer_crd_check.rc != 0

    - name: Wait for cert-manager webhook to be ready
      command: >-
        kubectl wait --for=condition=Available deployment
        -l app.kubernetes.io/name=cert-manager
        -n cert-manager --timeout=180s
      register: cert_manager_webhook_ready
      retries: 2
      delay: 20
      until: cert_manager_webhook_ready.rc == 0
      failed_when: false
      changed_when: false

    - name: Check webhook pod status on failure
      command: kubectl get pods -n cert-manager -l app.kubernetes.io/name=cert-manager
      register: webhook_pods
      when: cert_manager_webhook_ready.rc != 0
      changed_when: false

    - name: Display webhook failure diagnostics
      debug:
        msg: |
          Cert-manager webhook failed to become ready after 3 minutes.
          
          Pod status:
          {{ webhook_pods.stdout }}
          
          Check logs with:
          kubectl logs -n cert-manager -l app.kubernetes.io/name=cert-manager --tail=50
      when: cert_manager_webhook_ready.rc != 0

    - name: Fail if webhook not ready
      fail:
        msg: "Cert-manager webhook failed to become ready. See diagnostics above."
      when: cert_manager_webhook_ready.rc != 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('certManager', {}).get('enabled', false) | bool
    - not (skip_cert_manager_operator | default(false))

# Wait for pre-existing cert-manager (e.g., installed by Tekton/OpenShift Pipelines) to be ready.
# This handles the case where cert-manager was detected but not installed via OLM.
# We skip the CSV check since there's no OLM subscription, but still wait for CRDs and webhook.
- name: Wait for pre-existing cert-manager on OpenShift
  block:
    - name: Wait for Certificate CRD to be available (pre-existing)
      command: kubectl get crd certificates.cert-manager.io
      register: cert_crd_check_existing
      retries: 30
      delay: 10
      until: cert_crd_check_existing.rc == 0
      changed_when: false

    - name: Wait for Issuer CRD to be available (pre-existing)
      command: kubectl get crd issuers.cert-manager.io
      register: issuer_crd_check_existing
      retries: 30
      delay: 10
      until: issuer_crd_check_existing.rc == 0
      changed_when: false

    - name: Wait for cert-manager webhook to be ready (pre-existing)
      shell: >-
        kubectl wait --for=condition=Available deployment
        -l app.kubernetes.io/name=cert-manager -n cert-manager --timeout=60s 2>/dev/null ||
        kubectl wait --for=condition=Available deployment
        -l app=cert-manager -n cert-manager --timeout=60s 2>/dev/null ||
        kubectl get pods -n cert-manager --field-selector=status.phase=Running -o name | grep -qE 'cert-manager-(controller|webhook|cainjector)'
      args:
        executable: /bin/bash
      register: cert_manager_webhook_ready_existing
      retries: 3
      delay: 30
      until: cert_manager_webhook_ready_existing.rc == 0
  when:
    - enable_openshift | default(false)
    - skip_cert_manager_operator | default(false)
    - cert_manager_existing_check.rc == 0

# TODO: Move TektonConfig patching to kagenti-operator
# The kagenti-operator creates PipelineRuns for AgentBuilds, so it has the
# context to know what Tekton settings are needed. The operator should:
# 1. Check if running on OpenShift (detect TektonConfig CRD)
# 2. Patch TektonConfig to set pipeline.set-security-context: true
# 3. This ensures proper fsGroup handling for PVC permissions in build pods
# For now, we patch it in the installer as a workaround.
#
# IMPORTANT: On HyperShift clusters, TektonConfig may be created without
# proper SCC defaults (spec.platforms.openshift.scc is empty), causing the
# Tekton operator to crash with nil pointer dereference in rbac.go.
# We MUST patch the SCC configuration BEFORE waiting for Shipwright, because
# Shipwright depends on TektonConfig being ready.
- name: Patch TektonConfig for OpenShift build permissions
  block:
    - name: Wait for TektonConfig to exist
      command: kubectl get tektonconfig config
      register: tektonconfig_check
      retries: 30
      delay: 10
      until: tektonconfig_check.rc == 0
      changed_when: false

    # On HyperShift clusters, TektonConfig may be created with empty
    # spec.platforms.openshift, causing operator crash. Check and patch early.
    - name: Check if TektonConfig has SCC defaults
      command: kubectl get tektonconfig config -o jsonpath='{.spec.platforms.openshift.scc.default}'
      register: tektonconfig_scc_check
      failed_when: false
      changed_when: false

    - name: Patch TektonConfig SCC defaults for HyperShift clusters
      kubernetes.core.k8s:
        api_version: operator.tekton.dev/v1alpha1
        kind: TektonConfig
        name: config
        definition:
          spec:
            platforms:
              openshift:
                scc:
                  default: "pipelines-scc"
        merge_type: merge
      retries: 5
      delay: 10
      register: tektonconfig_scc_patch
      until: tektonconfig_scc_patch is not failed
      when: tektonconfig_scc_check.stdout | default('') | length == 0

    - name: Wait for Tekton operator to recover after SCC patch
      pause:
        seconds: 10
      when: tektonconfig_scc_check.stdout | default('') | length == 0

    - name: Wait for Tekton operator webhook to be ready
      command: >-
        kubectl rollout status deployment/tekton-operator-webhook
        -n openshift-operators --timeout=120s
      register: tekton_webhook_ready
      retries: 5
      delay: 10
      until: tekton_webhook_ready.rc == 0
      changed_when: false
      failed_when: false

    - name: Patch TektonConfig to set security context and pruner
      kubernetes.core.k8s:
        api_version: operator.tekton.dev/v1alpha1
        kind: TektonConfig
        name: config
        definition:
          spec:
            pipeline:
              set-security-context: true
            # Pruner configuration is required by newer Tekton versions.
            # Without spec.pruner.resources, the webhook validation fails with:
            # "validation failed: missing field(s): spec.pruner.resources"
            pruner:
              resources:
                - taskrun
                - pipelinerun
              keep: 100
              schedule: "0 8 * * *"
        merge_type: merge
      retries: 5
      delay: 15
      register: tektonconfig_patch
      until: tektonconfig_patch is not failed

    # Wait for TektonConfig to become fully Ready before proceeding.
    # OpenShift Builds operator depends on TektonConfig being ready before
    # it will deploy Shipwright components. Without this wait, the Builds
    # operator will be stuck in a retry loop waiting for Tekton.
    - name: Wait for TektonConfig to be Ready
      block:
        - name: Wait for TektonConfig Ready condition
          command: >-
            kubectl get tektonconfig config
            -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
          register: tektonconfig_ready_status
          retries: 90
          delay: 10
          until: tektonconfig_ready_status.stdout == "True"
          changed_when: false
      rescue:
        - name: Get TektonConfig full status for debugging
          command: kubectl get tektonconfig config -o yaml
          register: tektonconfig_full_status
          failed_when: false

        - name: Get Tekton operator pod status
          command: kubectl get pods -n openshift-operators -l app=tekton-operator -o wide
          register: tekton_operator_pods
          failed_when: false

        - name: Get Tekton operator logs
          shell: kubectl logs -n openshift-operators -l app=tekton-operator --tail=100 2>&1 || echo "No logs available"
          register: tekton_operator_logs
          failed_when: false

        - name: Get Tekton pipelines controller status
          command: kubectl get pods -n openshift-pipelines -o wide
          register: tekton_pipelines_pods
          failed_when: false

        - name: Display TektonConfig failure diagnostics
          debug:
            msg: |
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
              TektonConfig Failed to Become Ready
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

              TEKTONCONFIG STATUS:
              {{ tektonconfig_full_status.stdout | default('unable to get status') }}

              TEKTON OPERATOR PODS:
              {{ tekton_operator_pods.stdout | default('unable to get pods') }}

              TEKTON OPERATOR LOGS (last 100 lines):
              {{ tekton_operator_logs.stdout | default('no logs') }}

              TEKTON PIPELINES PODS:
              {{ tekton_pipelines_pods.stdout | default('unable to get pods') }}
              ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

        - name: Fail with TektonConfig error
          fail:
            msg: "TektonConfig did not become Ready after 15 minutes. See diagnostics above."
  when: enable_openshift | default(false)

# Wait for OpenShift Builds operator to be ready.
# The operator is installed via OLM subscription in kagenti-deps.
# The OpenShiftBuild operand CR is created via Helm hook (post-install).
# Skip if the operator was already installed (detected earlier) or if component is disabled.
- name: Wait for OpenShift Builds operator on OpenShift
  block:
    - name: Wait for OpenShift Builds operator CSV to succeed
      command: >-
        kubectl get csv -n openshift-builds
        -o jsonpath='{.items[?(@.spec.displayName=="Builds for Red Hat OpenShift Operator")].status.phase}'
      register: builds_csv_status
      retries: 60
      delay: 10
      until: builds_csv_status.stdout == "Succeeded"
      changed_when: false

    - name: Wait for OpenShiftBuild CRD to be available (operator ready)
      command: kubectl get crd openshiftbuilds.operator.openshift.io
      register: openshiftbuild_crd_check
      retries: 30
      delay: 10
      until: openshiftbuild_crd_check.rc == 0
      changed_when: false

    # When skipOpenShiftBuildOperand is set, Ansible creates the operand after the CRD is ready
    # This is needed on HyperShift/fresh installs where the CRD doesn't exist when Helm runs
    # Retry on transient API timeouts (common with HyperShift remote control planes)
    - name: Create OpenShiftBuild operand (skipped if Helm hook created it)
      kubernetes.core.k8s:
        state: present
        definition:
          apiVersion: operator.openshift.io/v1alpha1
          kind: OpenShiftBuild
          metadata:
            name: cluster
          spec:
            sharedResource:
              state: Enabled
            shipwright:
              build:
                state: Enabled
      register: openshiftbuild_create
      retries: 10
      delay: 15
      until: openshiftbuild_create is succeeded

    - name: Check OpenShiftBuild operand status
      command: kubectl get openshiftbuild cluster -o jsonpath='{.status.conditions[?(@.type=="Ready")].status}'
      register: openshiftbuild_status
      retries: 60
      delay: 10
      until: openshiftbuild_status.stdout == "True"
      changed_when: false
      failed_when: false

    - name: Display OpenShiftBuild operand status
      debug:
        msg: "OpenShiftBuild operand status: {{ openshiftbuild_status.stdout | default('not ready') }}"
      when: openshiftbuild_status.rc != 0 or openshiftbuild_status.stdout != "True"

    # Verify the OpenShiftBuild operand has Shipwright enabled
    - name: Check OpenShiftBuild operand Shipwright spec
      command: kubectl get openshiftbuild cluster -o jsonpath='{.spec.shipwright.build.state}'
      register: openshiftbuild_shipwright_state
      changed_when: false
      failed_when: false

    - name: Display OpenShiftBuild Shipwright state
      debug:
        msg: |
          OpenShiftBuild operand Shipwright state: {{ openshiftbuild_shipwright_state.stdout | default('not found') }}
          Expected: Enabled

    - name: Fail if Shipwright is not enabled in operand
      fail:
        msg: |
          OpenShiftBuild operand does not have Shipwright enabled!
          Current state: {{ openshiftbuild_shipwright_state.stdout | default('not found') }}
          This may indicate the Helm hook did not run or the operand was created by another source.
          Check: kubectl get openshiftbuild cluster -o yaml
      when: openshiftbuild_shipwright_state.stdout != "Enabled"

    # The Ready condition might be True before Shipwright is fully deployed.
    # Wait explicitly for Shipwright deployments to be ready.
    # See: https://docs.openshift.com/builds/1.0/installing/installing-openshift-builds.html
    - name: Wait for Shipwright build controller deployment to exist
      command: kubectl get deployment/shipwright-build-controller -n openshift-builds
      register: shipwright_controller_exists
      retries: 60
      delay: 10
      until: shipwright_controller_exists.rc == 0
      changed_when: false
      failed_when: false

    - name: Display Shipwright deployment status on failure
      shell: |
        echo "=== OpenShiftBuild operand status ==="
        kubectl get openshiftbuild cluster -o yaml 2>&1 || echo "Not found"
        echo ""
        echo "=== Pods in openshift-builds namespace ==="
        kubectl get pods -n openshift-builds 2>&1 || echo "None"
        echo ""
        echo "=== Deployments in openshift-builds namespace ==="
        kubectl get deployments -n openshift-builds 2>&1 || echo "None"
        echo ""
        echo "=== Events in openshift-builds namespace ==="
        kubectl get events -n openshift-builds --sort-by='.lastTimestamp' 2>&1 | tail -20 || echo "None"
        echo ""
        echo "=== OpenShift Builds operator pod logs ==="
        kubectl logs -n openshift-builds deployment/openshift-builds-operator -c operator --tail=50 2>&1 || echo "No operator pod"
        echo ""
        echo "=== TektonConfig status (Shipwright depends on Tekton) ==="
        kubectl get tektonconfig config -o jsonpath='{.status.conditions}' 2>&1 | jq . 2>/dev/null || kubectl get tektonconfig config -o yaml 2>&1 || echo "TektonConfig not found"
        echo ""
        echo "=== OpenShift Pipelines operator pods ==="
        kubectl get pods -n openshift-operators -l name=openshift-pipelines-operator 2>&1 || echo "None"
        echo ""
        echo "=== OpenShift Pipelines CSV status ==="
        kubectl get csv -n openshift-operators -l operators.coreos.com/openshift-pipelines-operator-rh.openshift-operators 2>&1 || echo "No Pipelines CSV"
        echo ""
        echo "=== Tekton pods in openshift-pipelines ==="
        kubectl get pods -n openshift-pipelines 2>&1 || echo "None"
      args:
        executable: /bin/bash
      register: shipwright_debug_info
      when: shipwright_controller_exists.rc != 0
      changed_when: false
      failed_when: false

    - name: Fail if Shipwright controller deployment not found
      fail:
        msg: |
          Shipwright build controller deployment not found in openshift-builds namespace.
          The OpenShift Builds operator may not have deployed Shipwright components.

          Debug info:
          {{ shipwright_debug_info.stdout | default('No debug info available') }}
      when: shipwright_controller_exists.rc != 0

    - name: Wait for Shipwright build controller deployment to be ready
      command: >-
        kubectl rollout status deployment/shipwright-build-controller
        -n openshift-builds --timeout=300s
      register: shipwright_controller_status
      retries: 5
      delay: 30
      until: shipwright_controller_status.rc == 0
      changed_when: false

    - name: Wait for Shipwright build webhook deployment to be ready
      command: >-
        kubectl rollout status deployment/shipwright-build-webhook
        -n openshift-builds --timeout=300s
      register: shipwright_webhook_status
      retries: 5
      delay: 30
      until: shipwright_webhook_status.rc == 0
      changed_when: false

    - name: Wait for Shipwright Build CRD to be available
      command: kubectl get crd builds.shipwright.io
      register: build_crd_check
      retries: 60
      delay: 10
      until: build_crd_check.rc == 0
      changed_when: false
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('shipwright', {}).get('enabled', false) | bool
    - not (skip_builds_operator | default(false))

# Configure TektonConfig to use external cert-manager instead of bundled version.
# OpenShift Pipelines 1.12+ can be configured to use a cluster-wide cert-manager,
# preventing CRD conflicts between the Pipelines-internal cert-manager and the
# cert-manager Operator for Red Hat OpenShift (or other external installations).
# This is especially important when cert-manager is already installed by kagenti
# or detected from another source (e.g., pre-existing OLM installation).
- name: Configure TektonConfig to use external cert-manager
  block:
    - name: Check if TektonConfig exists
      command: kubectl get tektonconfig config -o name
      register: tektonconfig_exists
      failed_when: false
      changed_when: false

    - name: Get current TektonConfig spec
      command: kubectl get tektonconfig config -o jsonpath='{.spec}'
      register: tektonconfig_spec
      when: tektonconfig_exists.rc == 0
      changed_when: false
      failed_when: false

    - name: Patch TektonConfig to use external cert-manager
      kubernetes.core.k8s:
        api_version: operator.tekton.dev/v1alpha1
        kind: TektonConfig
        name: config
        definition:
          spec:
            # Disable internal cert-manager installation in OpenShift Pipelines
            # This prevents conflicts with external cert-manager operators
            params:
              - name: createRbacResource
                value: "true"
            # Note: targetNamespace is immutable after creation (defaults to openshift-pipelines)
        merge_type: merge
      retries: 3
      delay: 10
      register: tektonconfig_certmanager_patch
      until: tektonconfig_certmanager_patch is not failed
      when: tektonconfig_exists.rc == 0

    - name: Log TektonConfig cert-manager configuration
      debug:
        msg: >-
          TektonConfig has been patched to work with external cert-manager.
          If OpenShift Pipelines still installs bundled cert-manager, check the
          Pipelines Operator subscription channel or disable Tekton Results.
      when: tektonconfig_exists.rc == 0
  when:
    - enable_openshift | default(false)
    - skip_cert_manager_operator | default(false) or
      (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('certManager', {}).get('enabled', false) | bool

# Wait for Istio ambient mesh components to be ready on OpenShift.
# The Istio, IstioCNI, and ZTunnel CRs are created as helm post-install hooks,
# and ztunnel pods may start before istiod is ready, causing certificate issues.
- name: Wait for Istio ambient mesh to be ready on OpenShift
  block:
    - name: Wait for istiod deployment to be ready
      command: >-
        kubectl rollout status deployment/istiod
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}
        --timeout=300s
      register: istiod_ready
      retries: 3
      delay: 30
      until: istiod_ready.rc == 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('istio', {}).get('enabled', false) | bool

# On OpenShift AI clusters, there may be a pre-existing Istio (openshift-gateway) that
# creates CA ConfigMaps in all namespaces. To avoid CA conflicts between our istiod
# and openshift-gateway istiod, we use the "Shared Trust Pattern" - copying their CA
# to our istiod so both create identical ConfigMaps.
#
# TODO: Implement proper multi-mesh trust based on Istio deployment models.
# The current approach (copying CA secret) is a workaround. A cleaner solution
# would follow Istio's "Shared control plane (single-cluster)" or "Multi-primary"
# trust models as documented at:
# https://istio.io/latest/docs/ops/deployment/deployment-models/
#
# Options to consider:
# 1. Plug-in CA certificates: Configure both istiods to use the same external CA
#    https://istio.io/latest/docs/tasks/security/cert-management/plugin-ca-cert/
# 2. Cross-cluster trust with intermediate CAs: Use cacerts Secret with proper chain
# 3. External CA integration (cert-manager): Single source of truth for certificates
- name: Share CA with openshift-gateway Istio (OpenShift AI clusters)
  block:
    - name: Check if openshift-gateway Istio CA exists
      command: kubectl get secret istio-ca-secret -n openshift-ingress
      register: og_ca_check
      failed_when: false
      changed_when: false

    - name: Copy openshift-gateway CA to istio-system (Shared Trust Pattern)
      shell: |
        kubectl get secret istio-ca-secret -n openshift-ingress -o yaml | \
          sed 's/namespace: openshift-ingress/namespace: {{ (charts["kagenti-deps"] | default({})).get("values", {}).get("istio", {}).get("namespace", "istio-system") }}/' | \
          grep -v "resourceVersion:\|uid:\|creationTimestamp:\|ownerReferences:" | \
          kubectl apply -f -
      when: og_ca_check.rc == 0

    - name: Restart istiod to use shared CA
      command: >-
        kubectl rollout restart deployment/istiod
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}
      when: og_ca_check.rc == 0

    - name: Wait for istiod rollout with shared CA
      command: >-
        kubectl rollout status deployment/istiod
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}
        --timeout=120s
      when: og_ca_check.rc == 0

    - name: Delete old CA ConfigMaps to force recreation with shared CA
      command: >-
        kubectl delete configmap istio-ca-root-cert
        -n {{ item }} --ignore-not-found
      loop:
        - "{{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('ztunnel', {}).get('namespace', 'istio-ztunnel') }}"
        - "{{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('namespace', 'istio-system') }}"
        - "{{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('cni', {}).get('namespace', 'istio-cni') }}"
        - "{{ (charts['kagenti-deps'] | default({})).get('namespace', 'kagenti-system') }}"
      ignore_errors: true
      when: og_ca_check.rc == 0

    - name: Wait for istiod to recreate CA ConfigMaps with shared CA
      pause:
        seconds: 10
      when: og_ca_check.rc == 0

    - name: Restart ztunnel to pick up shared CA certificates
      command: >-
        kubectl rollout restart daemonset/ztunnel
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('ztunnel', {}).get('namespace', 'istio-ztunnel') }}
      when: og_ca_check.rc == 0

    - name: Wait for ztunnel rollout to complete
      command: >-
        kubectl rollout status daemonset/ztunnel
        -n {{ (charts['kagenti-deps'] | default({})).get('values', {}).get('istio', {}).get('ztunnel', {}).get('namespace', 'istio-ztunnel') }}
        --timeout=300s
      register: ztunnel_ready
      retries: 3
      delay: 30
      until: ztunnel_ready.rc == 0
      when: og_ca_check.rc == 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti-deps'] | default({})).get('values', {}).get('components', {}).get('istio', {}).get('enabled', false) | bool

#
#  Setup DNS for registry in kind
#
- name: setup kind registry dns
  ansible.builtin.import_tasks: 03_setup_kind_registry_dns.yaml

#
#  MCP Gateway
#
- name: Install/upgrade mcp-gateway chart
  kubernetes.core.helm:
    release_name: mcp-gateway
    chart_ref: oci://ghcr.io/kagenti/charts/mcp-gateway
    chart_version: "{{ charts.mcpGateway.version }}"
    release_namespace: "{{ charts.mcpGateway.namespace }}"
    create_namespace: true
    state: present
    wait: false
    timeout: "{{ helm_wait_timeout }}s"
    values: >-
         {{ (((charts['mcpGateway'] | default({})).get('values')) | default({}))
          | combine((global_values_merged | default({})), recursive=True)
          | combine((((secret_values.get('charts', {}) | default({})).get('mcpGateway', {}) ).get('values', {})) , recursive=True)
          | combine((secret_values.get('global', {}) | default({})), recursive=True)
          | combine({'openshift': enable_openshift}, recursive=True) }}
  when: charts.mcpGateway.enabled | default(false)

#
#  Install kagenti
#
- name: Compute latest kagenti tag 
  block:
    - name: Lookup latest tag from remote git
      shell: |
        git ls-remote --tags --sort="v:refname" https://github.com/kagenti/kagenti.git | tail -n1 | sed 's|.*refs/tags/||; s/\^{}//'
      register: kagenti_latest_tag_cmd
      changed_when: false

    - name: Set kagenti_latest_tag fact
      set_fact:
        kagenti_latest_tag: "{{ kagenti_latest_tag_cmd.stdout }}"
  when: charts.kagenti.enabled | default(false)

- name: Run helm dependency update for kagenti when chart is local and dependency_update requested
  command: helm dependency update "{{ (charts['kagenti'] | default({})).get('chart','') }}"
  args:
    chdir: "{{ playbook_dir }}"
  when:
    - (charts['kagenti'] | default({})).get('chart') is defined
    - (charts['kagenti'] | default({})).get('dependency_update', true) | bool
    - (charts['kagenti'] | default({})).get('chart') is search(local_chart_path_regex)
    - (charts['kagenti'] | default({})).get('enabled', false) | bool

# Build OpenShift-specific overrides for kagenti chart
# This sets the SPIFFE prefix using the detected trust domain
- name: Build OpenShift overrides for kagenti chart
  block:
    - name: Initialize kagenti overrides
      set_fact:
        kagenti_openshift_overrides: {}

    # Set agentOAuthSecret.spiffePrefix to use the detected trust domain
    # This follows the convention: spiffe://<trust-domain>/sa
    - name: Add agentOAuthSecret.spiffePrefix override
      set_fact:
        kagenti_openshift_overrides: "{{ kagenti_openshift_overrides | combine({'agentOAuthSecret': {'spiffePrefix': 'spiffe://' + ocp_trust_domain + '/sa'}}, recursive=True) }}"
      when: ocp_trust_domain is defined and ocp_trust_domain | trim | length > 0

    - name: Debug kagenti OpenShift overrides
      debug:
        msg: "Kagenti OpenShift overrides: {{ kagenti_openshift_overrides }}"
      when: kagenti_openshift_overrides | length > 0
  when:
    - enable_openshift | default(false)
    - (charts['kagenti'] | default({})).get('enabled', false) | bool

- name: Install/upgrade Kagenti chart and print instructions
  block:
  # Delete old OAuth secret jobs BEFORE Helm install to allow fresh job creation.
  # Jobs are immutable, so Helm can't update them - we must delete first.
  - name: Delete old kagenti-ui-oauth-secret-job before Helm install
    kubernetes.core.k8s:
      state: absent
      api_version: batch/v1
      kind: Job
      name: kagenti-ui-oauth-secret-job
      namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"
    failed_when: false

  - name: Delete old kagenti-agent-oauth-secret-job before Helm install
    kubernetes.core.k8s:
      state: absent
      api_version: batch/v1
      kind: Job
      name: kagenti-agent-oauth-secret-job
      namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"
    failed_when: false

  - name: Install/upgrade Kagenti chart
    kubernetes.core.helm:
      release_name: "{{ (charts['kagenti'] | default({})).get('release_name', 'kagenti') }}"
      chart_ref: "{{ (charts['kagenti'] | default({})).get('chart') }}"
      chart_repo_url: "{{ (charts['kagenti'] | default({})).get('repo_url', omit) }}"
      kubeconfig: ""
      release_namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"
      state: present
      create_namespace: false
      wait: true
      timeout: "{{ helm_wait_timeout }}s"
      values: >-
        {{ (((charts['kagenti'] | default({})).get('values')) | default({}))
          | combine({'ui': {'frontend': {'tag': kagenti_latest_tag}, 'backend': {'tag': kagenti_latest_tag}}}, recursive=True)
          | combine((global_values_merged | default({})), recursive=True)
          | combine((((secret_values.get('charts', {}) | default({})).get('kagenti', {}) ).get('values', {})) , recursive=True)
          | combine((secret_values.get('global', {}) | default({})), recursive=True)
          | combine({'openshift': enable_openshift}, recursive=True)
          | combine((kagenti_openshift_overrides | default({})), recursive=True) }}
      values_files: >
        {{ (global_value_files | default([]) | list) +
          (((charts['kagenti'] | default({}))['values_files'] | default([])) | list)
        }}

  # TODO: Move github-clone-step fixes to kagenti-operator.
  # The kagenti-operator creates the github-clone-step ConfigMap for Tekton pipelines.
  # On OpenShift with Istio ambient mode:
  # 1. HOME must be set to /tekton/home (git-init can't write to / on OpenShift)
  # 2. A 30s delay is needed for ztunnel to register the workload before DNS queries
  #    (ztunnel has a 5s timeout waiting for xds workload info from istiod)
  # The operator should include these fixes in the ConfigMap definition.
  # For now, we patch it in the installer as a workaround.
  - name: Wait for kagenti-operator to create github-clone-step ConfigMap
    command: kubectl get configmap github-clone-step -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}
    register: github_clone_step_check
    retries: 30
    delay: 5
    until: github_clone_step_check.rc == 0
    changed_when: false
    when: enable_openshift | default(false)

  - name: Patch github-clone-step ConfigMap for OpenShift and Istio ambient mode
    kubernetes.core.k8s:
      api_version: v1
      kind: ConfigMap
      name: github-clone-step
      namespace: "{{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}"
      definition:
        data:
          task-spec.yaml: |
            params:
              - name: GITHUB_TOKEN
                type: string
                description: The Git repository URL with the token
                default: ""
              - name: repo-url
                type: string
                description: The Git repository URL without the token
              - name: revision
                type: string
                description: The Git revision to checkout
                default: "main"
            steps:
              - name: git-clone
                image: ghcr.io/tektoncd/github.com/tektoncd/pipeline/cmd/git-init:latest
                env:
                  - name: HOME
                    value: /tekton/home
                script: |
                  #!/bin/sh
                  # Wait for Istio ambient mesh ztunnel to register this workload.
                  # Without this delay, DNS queries fail because ztunnel times out
                  # waiting for workload info from istiod (xds).
                  echo "Waiting 30s for Istio ambient mesh to register this workload..."
                  sleep 30
                  /ko-app/git-init -url https://$(params.repo-url) -revision $(params.revision) -path /workspace/source
              - name: list-dir
                image: public.ecr.aws/docker/library/alpine:latest
                workingDir: $(workspaces.source.path)
                script: |
                    echo "Listing contents of workspace root ($(workspaces.source.path)): "
                    ls -la

            workspaces:
              - name: source
      merge_type: merge
    when: enable_openshift | default(false)

  when:
  - (charts['kagenti'] | default({})).get('enabled', false) | bool
  - (charts['kagenti'] | default({})).get('chart') is defined

# TODO: Move internal-registry-secret creation to kagenti-operator or kagenti helm chart.
# The kagenti-operator creates AgentBuilds with Tekton pipelines that push images
# to the OpenShift internal registry. The buildah step needs a dockerconfigjson-format
# secret with registry credentials. OpenShift auto-creates builder-dockercfg secrets
# but in dockercfg format, which buildah cannot use directly.
# For now, we create the internal-registry-secret in each agent namespace as a workaround.
- name: Create internal-registry-secret in agent namespaces for OpenShift
  block:
    - name: Create internal-registry-secret from builder SA token
      shell: |
        for ns in {{ (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | join(' ') }}; do
          # Get builder dockercfg and convert to dockerconfigjson format
          DOCKERCFG=$(kubectl get secret -n $ns -o name | grep builder-dockercfg | head -1 | xargs -I {} kubectl get {} -n $ns -o jsonpath='{.data.\.dockercfg}' | base64 -d)
          if [ -n "$DOCKERCFG" ]; then
            DOCKERCONFIGJSON=$(echo "{\"auths\": $DOCKERCFG}" | base64 | tr -d '\n')
            kubectl apply -f - <<EOF
        apiVersion: v1
        kind: Secret
        metadata:
          name: internal-registry-secret
          namespace: $ns
        type: kubernetes.io/dockerconfigjson
        data:
          .dockerconfigjson: $DOCKERCONFIGJSON
        EOF
            echo "Created internal-registry-secret in $ns"
          else
            echo "No builder-dockercfg found in $ns, skipping"
          fi
        done
      args:
        executable: /bin/bash
  when:
    - enable_openshift | default(false)
    - (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | length > 0

# TODO: Move spiffe-helper-config ConfigMap creation to kagenti-operator.
# Agent/tool workloads with SPIFFE identity injection expect this ConfigMap for the spiffe-helper sidecar.
# For now, we create it in each agent namespace.
- name: Create spiffe-helper-config ConfigMap in agent namespaces
  block:
    - name: Apply spiffe-helper-config ConfigMap
      shell: |
        for ns in {{ (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | join(' ') }}; do
          kubectl apply -n $ns -f - <<EOF
        apiVersion: v1
        kind: ConfigMap
        metadata:
          name: spiffe-helper-config
        data:
          helper.conf: |
            agent_address = "/spiffe-workload-api/spire-agent.sock"
            cmd = ""
            cmd_args = ""
            svid_file_name = "/opt/svid.pem"
            svid_key_file_name = "/opt/svid_key.pem"
            svid_bundle_file_name = "/opt/svid_bundle.pem"
            jwt_svids = [{jwt_audience="kagenti", jwt_svid_file_name="/opt/jwt_svid.token"}]
            jwt_svid_file_mode = 0644
            include_federated_domains = true
        EOF
          echo "Created spiffe-helper-config in $ns"
        done
      args:
        executable: /bin/bash
  when:
    - (((charts['kagenti'] | default({})).get('values')) | default({})).get('agentNamespaces', []) | length > 0

# TODO: Move OAuth secret job ordering to kagenti helm chart.
# The kagenti-ui-oauth-secret-job needs the OpenShift Route to exist before it runs,
# because it reads the Route host to construct the OAuth callback URL. Currently,
# the Job and Route are created in parallel, causing a race condition where the Job
# may fail with "route not found" error. The Job eventually succeeds on retry, but
# the UI deployment may have already started with missing/stale OAuth credentials.
#
# Proper fix in charts/kagenti/templates/:
# 1. Add helm hook annotation to oauth-secret-job.yaml:
#    "helm.sh/hook": post-install,post-upgrade
#    "helm.sh/hook-weight": "10"  # Higher than Route weight
# 2. Ensure Route has lower hook-weight or is a regular resource (not a hook)
# 3. This guarantees Route exists before Job runs
#
# For now, we restart the UI deployment after the chart installs to pick up the
# OAuth secret that was created by the (eventually successful) Job.
- name: Restart kagenti-ui to pick up OAuth secret on OpenShift
  block:
    - name: Wait for OAuth secret job to complete
      command: >-
        kubectl wait --for=condition=complete job
        -l app.kubernetes.io/component=oauth-secret
        -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}
        --timeout=120s
      register: oauth_job_wait
      failed_when: false
      changed_when: false

    - name: Restart kagenti-ui deployment
      command: >-
        kubectl rollout restart deployment/kagenti-ui
        -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}

    - name: Wait for kagenti-ui rollout to complete
      command: >-
        kubectl rollout status deployment/kagenti-ui
        -n {{ (charts['kagenti'] | default({})).get('namespace', 'kagenti-system') }}
        --timeout=120s
  when:
    - enable_openshift | default(false)
    - (charts['kagenti'] | default({})).get('enabled', false) | bool

